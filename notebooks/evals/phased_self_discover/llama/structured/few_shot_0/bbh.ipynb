{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyprojroot import here\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(\"evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "t4d = (\n",
    "    lambda y_i, y_pred_i: y_pred_i\n",
    "    and y_i in y_pred_i\n",
    "    and y_i == str(y_pred_i.translate(str.maketrans(\"\", \"\", \".'\"))[2:])\n",
    ")\n",
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\\\"\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.(),\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correct_prediction_count(benchmark, y: list[str], y_pred: list[str]):\n",
    "    correct_preds = 0\n",
    "    for y_i, y_pred_i in tqdm(zip(y, y_pred), desc=\"Calculating...\"):\n",
    "        if benchmark == \"t4d\":\n",
    "            eval_fn = t4d\n",
    "        elif benchmark == \"bbh\":\n",
    "            eval_fn = bbh\n",
    "\n",
    "        if eval_fn(y_i, y_pred_i):\n",
    "            correct_preds += 1\n",
    "        else:\n",
    "            print(y_i)\n",
    "            print(\"-\" * 100)\n",
    "            print(y_pred_i)\n",
    "            print(\"-+=\" * 100)\n",
    "    return correct_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# boolean_expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'boolean_expressions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-boolean_expressions/bbh-boolean_expressions_eval')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the filled JSON for the given reasoning structure:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Understanding the Statement\": {\n",
      "        \"Break down the statement into its basic components\": {\n",
      "            \"Identify the logical operators used\": \"AND, OR, NOT\",\n",
      "            \"Identify the truth values involved\": \"True, False\"\n",
      "        },\n",
      "        \"Define the truth values\": {\n",
      "            \"Definition of True\": \"A statement that is correct or accurate\",\n",
      "            \"Definition of False\": \"A statement that is incorrect or inaccurate\"\n",
      "        }\n",
      "    },\n",
      "    \"Analyzing the Negation Operator\": {\n",
      "        \"Understanding the effect of 'not True'\": {\n",
      "            \"Applying the negation operator to True\": \"NOT True\",\n",
      "            \"Resulting truth value\": \"False\"\n",
      "        }\n",
      "    },\n",
      "    \"Applying Logical Rules and Operators\": {\n",
      "        \"Understanding the OR operator\": {\n",
      "            \"Effect of OR on True and False\": \"Returns True if at least one of the operands is True\",\n",
      "            \"Effect of OR on (not True)\": \"Returns False if the other operand is False\"\n",
      "        },\n",
      "        \"Combining the truth values using OR\": {\n",
      "            \"First, evaluate the AND operator\": \"True AND False = False\",\n",
      "            \"Then, evaluate the OR operator\": \"False OR False = False\",\n",
      "            \"Resulting truth value\": \"False\"\n",
      "        }\n",
      "    },\n",
      "    \"Evaluating the Statement\": {\n",
      "        \"Combining the results of the previous steps\": {\n",
      "            \"Final truth value\": \"False\"\n",
      "        }\n",
      "    },\n",
      "    \"Conclusion\": {\n",
      "        \"Is the final truth value True or False?\": \"False\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is False.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e79ff9a16c54831a5c66474de6bc896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False, True.\n",
      "\n",
      "False, correct based on the order of operations and boolean logic rules\"\n",
      "\n",
      "True, False.\n",
      "\n",
      "True, False.\n",
      "\n",
      "True, False.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.984"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"]) + 1) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'causal_judgement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/mistral/structured/few_shot_5/bbh/bbh-causal_judgement/bbh-causal_judgement_eval')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'few_shot_examples', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 187\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"Step 1: Identify the primary event or action that triggered the problem\": {\n",
      "        \"Description\": \"Determine the main event that led to the issue.\",\n",
      "        \"Action\": \"Identify the simultaneous login of Alice and Zoe at 9 am.\"\n",
      "    },\n",
      "    \"Step 2: Identify the unspoken rules or conditions that led to this situation\": {\n",
      "        \"Description\": \"Understand the underlying rules or conditions that caused the problem.\",\n",
      "        \"Action\": \"Recognize the rule that an empty email is sent if two people are logged in at the same time.\"\n",
      "    },\n",
      "    \"Step 3: Sequence the events leading up to the problem\": {\n",
      "        \"Description\": \"List the events in chronological order.\",\n",
      "        \"Action\": [\n",
      "            \"Alice logs in at 9 am.\",\n",
      "            \"Zoe logs in at 9 am.\",\n",
      "            \"An empty email is sent immediately.\"\n",
      "        ]\n",
      "    },\n",
      "    \"Step 4: Analytical Thinking - Evaluate the problem from different viewpoints\": {\n",
      "        \"Description\": \"Consider different perspectives and challenge assumptions.\",\n",
      "        \"Action\": [\n",
      "            \"Consider the perspective of Alice.\",\n",
      "            \"Consider the perspective of Zoe.\",\n",
      "            \"Consider the perspective of the system rules.\"\n",
      "        ]\n",
      "    },\n",
      "    \"Step 5: Identify the specific actions or conditions that directly led to the problem\": {\n",
      "        \"Description\": \"Pinpoint the exact actions that caused the issue.\",\n",
      "        \"Action\": \"Identify the simultaneous login of Alice and Zoe as the direct cause.\"\n",
      "    },\n",
      "    \"Step 6: Determine if the problem is influenced by human actions or behaviors\": {\n",
      "        \"Description\": \"Assess if human actions played a role in the problem.\",\n",
      "        \"Action\": \"Evaluate the timing of logins by Alice and Zoe.\"\n",
      "    },\n",
      "    \"Step 7: Break down the events leading up to the problem step by step\": {\n",
      "        \"Description\": \"Detail each step that led to the problem.\",\n",
      "        \"Action\": [\n",
      "            \"Step 1: Alice logs in at 9 am.\",\n",
      "            \"Step 2: Zoe logs in at 9 am.\",\n",
      "            \"Step 3: An empty email is sent due to simultaneous login.\"\n",
      "        ]\n",
      "    },\n",
      "    \"Step 8: Conclusion - Did Zoe cause an empty email to be sent?\": {\n",
      "        \"Description\": \"Based on the analysis, determine if Zoe's action caused the problem.\",\n",
      "        \"Action\": \"Evaluate if Zoe's login at the same time as Alice directly led to the email being sent.\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is Yes.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\"'))[:3].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating...: 187it [00:00, 29266.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, Yes.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "No, Yes\"\n",
      "\n",
      "No, Yes.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "No, Yes.\n",
      "\n",
      "No, Yes.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "No, Yes.\n",
      "\n",
      "No, Yes.\"\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "No, Yes.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "No, Yes.\n",
      "\n",
      "No, Yes.\n",
      "\n",
      "No, Yes.\n",
      "\n",
      "No, Yes.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "No, Yes.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "No, Yes.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "No, Yes.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "No, Yes.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n",
      "Yes, No.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.732620320855615"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# date_understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'date_understanding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-date_understanding/bbh-date_understanding_eval')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "    \"Step 1: Identify the specific date information given in the task\": {\n",
      "        \"Date given in the task\": \"Jan 21, 2011\",\n",
      "        \"Relevant information for date calculation\": \"yesterday's date is given, and we need to find the date one week ago from today\"\n",
      "    },\n",
      "    \"Step 2: Eliminate irrelevant information\": {\n",
      "        \"Irrelevant information\": \"Jane ate 2 pizzas and 5 wings\",\n",
      "        \"Relevant information for date calculation\": \"Jan 21, 2011, and the fact that yesterday's date is given\"\n",
      "    },\n",
      "    \"Step 3: Identify the individual components of the date\": {\n",
      "        \"Month\": \"January\",\n",
      "        \"Day\": \"21\",\n",
      "        \"Year\": \"2011\"\n",
      "    },\n",
      "    \"Step 4: Determine the reference point for the calculation\": {\n",
      "        \"Exact date given in the task\": \"Jan 21, 2011 (yesterday's date)\",\n",
      "        \"Reference point for calculation\": \"today's date, which is Jan 22, 2011\"\n",
      "    },\n",
      "    \"Step 5: Calculate the date one week ago\": {\n",
      "        \"Starting date\": \"Jan 22, 2011\",\n",
      "        \"Subtract one week\": \"7 days\",\n",
      "        \"Resulting date\": \"Jan 15, 2011\"\n",
      "    },\n",
      "    \"Step 6: Format the resulting date in MM/DD/YYYY\": {\n",
      "        \"Month\": \"01\",\n",
      "        \"Day\": \"15\",\n",
      "        \"Year\": \"2011\",\n",
      "        \"Formatted date\": \"01/15/2011\"\n",
      "    },\n",
      "    \"Step 7: Compare the resulting date with the options\": {\n",
      "        \"Resulting date\": \"01/15/2011\",\n",
      "        \"Matching option\": \"B\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is B.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\"'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf29ea3a98714e35b307091a4945a3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(C), B.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(B), None\n",
      "\n",
      "(F), A.\n",
      "\n",
      "(E), B.\n",
      "\n",
      "(D), E.\n",
      "\n",
      "(E), B.\n",
      "\n",
      "(A), 11/29/2001.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(C), 08/25/2021.\n",
      "\n",
      "(F), C.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(E), D.\n",
      "\n",
      "(D), 12/02/1962.\n",
      "\n",
      "(D), None of the options.\n",
      "\n",
      "(F), B.\n",
      "\n",
      "(D), F.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(F), 10/22/2001\n",
      "\n",
      "(D), F.\n",
      "\n",
      "(D), 07/10/1972.\n",
      "\n",
      "(E), C.\n",
      "\n",
      "(E), D.\n",
      "\n",
      "(C), None of the given options.\n",
      "\n",
      "(F), E.\n",
      "\n",
      "(B), D.\n",
      "\n",
      "(B), F.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(C), F.\n",
      "\n",
      "(C), F.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.876"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cd45f10d064b3aa32721d31d7fba47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jane quited her job on Mar 20, 2020. 176 days have passed since then. What is the date yesterday in MM/DD/YYYY?\n",
      "Options:\n",
      "(A) 09/11/2094\n",
      "(B) 09/11/2020\n",
      "(C) 09/10/2020\n",
      "(D) 08/14/2020\n",
      "(E) 10/09/2020\n",
      "(F) 09/17/2020\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(B) None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Here's the filled JSON for the given reasoning structure:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Step 1: Identify the initial date and the number of days passed\": {\n",
      "        \"Initial date\": \"March 20, 2020\",\n",
      "        \"Number of days passed\": 176\n",
      "    },\n",
      "    \"Step 2: Break down the number of days into smaller parts\": {\n",
      "        \"Number of weeks\": 25,\n",
      "        \"Number of remaining days\": 1\n",
      "    },\n",
      "    \"Step 3: Calculate the number of months and remaining days\": {\n",
      "        \"Number of months\": 6,\n",
      "        \"Number of remaining days\": 0\n",
      "    },\n",
      "    \"Step 4: Determine the new month and day after adding the calculated months and days\": {\n",
      "        \"New month\": \"September\",\n",
      "        \"New day\": 8\n",
      "    },\n",
      "    \"Step 5: Consider the case where the new month is in the next year\": {\n",
      "        \"Is the new month in the next year?\": \"No\",\n",
      "        \"New year\": 2020\n",
      "    },\n",
      "    \"Step 6: Calculate the date yesterday\": {\n",
      "        \"Date yesterday (MM/DD/YYYY)\": \"09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020\"\n",
      "    },\n",
      "    \"Step 7: Compare the calculated date with the given options\": {\n",
      "        \"Is the calculated date among the options?\": \"No but closest is (C) 09/10/2020 - 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/2020, yesterday of 09/09/2020 is 09/08/2020 + 1 day = 09/09/202\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "In the US, Thanksgiving is on the fourth Thursday of November. Today is the US Thanksgiving of 2001. What is the date one week from today in MM/DD/YYYY?\n",
      "Options:\n",
      "(A) 11/29/2002\n",
      "(B) 11/15/2002\n",
      "(C) 11/30/2002\n",
      "(D) 12/27/2002\n",
      "(E) 12/11/2002\n",
      "(F) 11/29/2078\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(A) 11/29/2001.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "It was Sept. 1st, 2021 a week ago. What is the date one week ago from today in MM/DD/YYYY?\n",
      "Options:\n",
      "(A) 09/02/2021\n",
      "(B) 09/07/2021\n",
      "(C) 09/01/2021\n",
      "(D) 09/01/2098\n",
      "(E) 09/01/2020\n",
      "(F) 12/06/2021\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(C) 08/25/2021.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "Jane and John married on Jan 2, 1958. It is their 5-year anniversary today. What is the date a month ago in MM/DD/YYYY?\n",
      "Options:\n",
      "(A) 12/02/2000\n",
      "(B) 12/02/2055\n",
      "(C) 12/01/1960\n",
      "(D) 12/02/1960\n",
      "(E) 05/02/1961\n",
      "(F) 11/18/1960\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(D) 12/02/1962.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "Jane and John married on Jan 2, 1958. It is their 5-year anniversary today. What is the date one week ago from today in MM/DD/YYYY?\n",
      "Options:\n",
      "(A) 01/09/1961\n",
      "(B) 01/02/1961\n",
      "(C) 10/01/1960\n",
      "(D) 12/26/1960\n",
      "(E) 07/26/1960\n",
      "(F) 12/26/1936\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(D) None of the options.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "In the US, Thanksgiving is on the fourth Thursday of November. Today is the US Thanksgiving of 2001. What is the date a month ago in MM/DD/YYYY?\n",
      "Options:\n",
      "(A) 11/22/2001\n",
      "(B) 10/21/2002\n",
      "(C) 07/21/2002\n",
      "(D) 10/22/1923\n",
      "(E) 10/15/2002\n",
      "(F) 10/22/2002\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(F) 10/22/2001\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "Jane scheduled 3 apointments with 5 poeple for tomorrow (Tue, 7/9/1972). What is the date tomorrow in MM/DD/YYYY?\n",
      "Options:\n",
      "(A) 07/09/1923\n",
      "(B) 08/05/1972\n",
      "(C) 09/01/1972\n",
      "(D) 07/09/1972\n",
      "(E) 07/09/2007\n",
      "(F) 09/04/1972\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(D) 07/10/1972.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "Today is 9/7. Jane is watching NFL 2003. What is the date today in MM/DD/YYYY?\n",
      "Options:\n",
      "(A) 12/05/2003\n",
      "(B) 08/08/2003\n",
      "(C) 07/07/2003\n",
      "(D) 09/07/2003\n",
      "(E) 11/20/2003\n",
      "(F) 09/06/2003\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(D) (D).\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "On May 9th, 2017 Jane bought 40 eggs. She ate one per day. Today she ran out of eggs. What is the date yesterday in MM/DD/YYYY?\n",
      "Options:\n",
      "(A) 07/18/2017\n",
      "(B) 06/17/2017\n",
      "(C) 06/17/2111\n",
      "(D) 07/13/2017\n",
      "(E) 06/26/2017\n",
      "(F) 06/25/2017\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(B) (B) 06/17/2017.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "In the US, Thanksgiving is on the fourth Thursday of November. Today is the US Thanksgiving of 2001. What is the date yesterday in MM/DD/YYYY?\n",
      "Options:\n",
      "(A) 09/12/2002\n",
      "(B) 11/30/2002\n",
      "(C) 11/21/2002\n",
      "(D) 11/21/2076\n",
      "(E) 11/07/2002\n",
      "(F) 11/15/2002\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(C) None of the given options.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_fn(ins):\n",
    "    if not ins[\"answer_pred\"] or len(ins[\"answer_pred\"]) > 4:\n",
    "        print(ins[\"input\"])\n",
    "        print(\"-\" * 100)\n",
    "        print(ins[\"target\"], ins[\"answer_pred\"])\n",
    "        print(\"-\" * 100)\n",
    "        if not ins[\"answer_pred\"]:\n",
    "            print(ins[\"trajectory\"])\n",
    "        print(\"-+\" * 100)\n",
    "\n",
    "dataset.map(map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'disambiguation_qa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-disambiguation_qa/bbh-disambiguation_qa_eval')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "    \"Step 1: Identify the pronoun and its potential antecedents\": {\n",
      "        \"Pronoun\": \"they\",\n",
      "        \"Potential antecedents\": [\"The worker\", \"The pedestrian\"]\n",
      "    },\n",
      "    \"Step 2: Analyze the sentence structure to understand the pronoun's relationship with potential antecedents\": {\n",
      "        \"Sentence structure\": \"The worker told the pedestrian that they were repairing the sidewalk as quickly as possible.\",\n",
      "        \"Relationship between pronoun and potential antecedents\": \"The pronoun 'they' is the subject of the subordinate clause, and its antecedent could be either 'The worker' or 'The pedestrian'.\"\n",
      "    },\n",
      "    \"Step 3: Examine the grammatical evidence to support or refute each potential antecedent\": {\n",
      "        \"Grammatical evidence for each potential antecedent\": [\n",
      "            {\"Antecedent\": \"The worker\", \"Evidence\": \"The worker is the main subject of the sentence, and the verb 'told' is in the active voice, suggesting that the worker is performing the action.\"},\n",
      "            {\"Antecedent\": \"The pedestrian\", \"Evidence\": \"The pedestrian is the indirect object of the sentence, and the verb 'told' is in the active voice, suggesting that the pedestrian is receiving the information.\"}\n",
      "        ]\n",
      "    },\n",
      "    \"Step 4: Evaluate the contextual clues to support or refute each potential antecedent\": {\n",
      "        \"Contextual clues for each potential antecedent\": [\n",
      "            {\"Antecedent\": \"The worker\", \"Clues\": \"The worker is repairing the sidewalk, which is a common task for workers.\"},\n",
      "            {\"Antecedent\": \"The pedestrian\", \"Clues\": \"The pedestrian is not typically responsible for repairing sidewalks.\"}\n",
      "        ]\n",
      "    },\n",
      "    \"Step 5: Consider alternative perspectives and question assumptions about the pronoun's antecedent\": {\n",
      "        \"Alternative perspectives\": [\n",
      "            {\"Perspective\": \"The pronoun 'they' could refer to a group of workers, not just the individual worker mentioned in the sentence.\"},\n",
      "            {\"Perspective\": \"The pronoun 'they' could be used in a generic sense, referring to the organization or company responsible for repairing the sidewalk.\"}\n",
      "        ],\n",
      "        \"Assumptions questioned\": [\n",
      "            {\"Assumption\": \"The assumption that the pronoun 'they' must refer to a specific individual mentioned in the sentence.\"},\n",
      "            {\"Assumption\": \"The assumption that the pronoun 'they' cannot be used in a generic sense.\"}\n",
      "        ]\n",
      "    },\n",
      "    \"Step 6: Assess the ambiguity of the pronoun's antecedent based on the analysis\": {\n",
      "        \"Is the pronoun's antecedent ambiguous?\": \"Yes\"\n",
      "    },\n",
      "    \"Step 7: Determine the most likely antecedent of the pronoun or conclude that it is ambiguous\": {\n",
      "        \"Most likely antecedent or ambiguity conclusion\": \"Ambiguous\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is C.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\"'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0615bf6f0e4bff90322c1547f96e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A), C.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(A), (C) Ambiguous.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(B), option (B) because the sentence's structure and the use of the word 'bought' imply that the accountant is the recipient of the car and therefore the one who needs it. This interpretation is consistent with standard grammatical rules and the logic of the situation.\"\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(A), (C) Ambiguous.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(B), C.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.788"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"]) + 1) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dyck_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'dyck_languages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-dyck_languages/bbh-dyck_languages_eval')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the given task, we will follow the provided step-by-step reasoning plan in JSON format.\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Step 1: Analyze the input sequence\": {\n",
      "        \"Input sequence\": \"{ ( < [ < > ]\",\n",
      "        \"Identify the types of parentheses\": [\"{\", \"(\", \"<\", \"[\", \">\"],\n",
      "        \"Count the number of opening parentheses\": {\n",
      "            \"{\": 1,\n",
      "            \"(\": 1,\n",
      "            \"<\": 2,\n",
      "            \"[\": 1\n",
      "        }\n",
      "    },\n",
      "    \"Step 2: Identify matching pairs of parentheses\": {\n",
      "        \"List of matching pairs\": [\"{ }\", \"( )\", \"< >\", \"[ ]\"],\n",
      "        \"Identify pairs that can be closed immediately\": [\"< >\"]\n",
      "    },\n",
      "    \"Step 3: Break down the sequence into smaller parts\": {\n",
      "        \"Focus on one type of parenthesis at a time\": [\"{\", \"(\", \"<\", \"[\", \"]\"],\n",
      "        \"Start with innermost parentheses\": \"< >\"\n",
      "    },\n",
      "    \"Step 4: Explore possible closing sequences\": {\n",
      "        \"List of possible closing sequences\": [\"] >\", \") >\", \"} >\", \"] )\", \"} )\", \"] ) >\", \"} ) >\"],\n",
      "        \"Apply each sequence to the input\": [\n",
      "            \"{ ( < [ < > ] >\", \n",
      "            \"{ ( < [ < > ) >\", \n",
      "            \"{ ( < [ < > } >\", \n",
      "            \"{ ( < [ < > ] )\", \n",
      "            \"{ ( < [ < > } )\", \n",
      "            \"{ ( < [ < > ] ) >\", \n",
      "            \"{ ( < [ < > } ) >\"\n",
      "        ]\n",
      "    },\n",
      "    \"Step 5: Simplify the sequence by closing matching pairs\": {\n",
      "        \"Close matching pairs\": [\"< >\"],\n",
      "        \"Update the sequence\": \"{ ( [ ]\"\n",
      "    },\n",
      "    \"Step 6: Repeat steps 3-5 until the sequence is complete\": {\n",
      "        \"Continue breaking down the sequence\": [\"{\", \"(\", \"[\", \"]\"],\n",
      "        \"Apply creative thinking and explore unconventional patterns\": [\n",
      "            \"Close matching pairs [ ]\", \n",
      "            \"Update the sequence: { ( )\"\n",
      "        ]\n",
      "    },\n",
      "    \"Step 7: Finalize the completed sequence\": {\n",
      "        \"Completed sequence\": \"{ ( [ < > ] ) }\",\n",
      "        \"Verify proper closure\": [\"{ }\", \"( )\", \"[ ]\", \"< >\"]\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "After completing the above reasoning plan, the final answer is: \n",
      "\n",
      "The final answer is { ( [ < > ] ) }.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e04f8e17424f3d83175e06a830b278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_fn(ins):\n",
    "    find = \"Input: \"\n",
    "    index = ins[\"input\"].find(find)\n",
    "    \n",
    "    return {\n",
    "        \"target\": ins[\"input\"][index + len(find):] + \" \" + ins[\"target\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\\\"\")\n",
    ").replace(\" \", \"\") == y_pred_i.translate(str.maketrans(\"\", \"\", '.(),\"')).replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e49ccaff08c4f76aaee0be1b47410c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ ( < [ < > ] > ) }, { ( [ < > ] ) }.\n",
      "\n",
      "{ ( < > ) } ( ( [ ] ) < [ ( [ [ ] ] [ { } ] { } [ < { [ ] } > ] ( ) ) ] > ), { ( < > ) } ( ( ) < [ ( [ [ ] ] [ { } ] { } [ < { [ ] } > ] ( ) ) ] ).\n",
      "\n",
      "< ( < { [ { } < ( { ( < < < { [ ( [ ( [ { { < [ { } < ( ) > ] > } } ] ) ] ) ] } > < > > ( ( < { } > ) ) > ) } ) > ] } > ) >, < ( < { [ { } < ( { ( < < < { [ ( [ ( [ { { < [ { } < ( ) > ] > } } ] ) ] ) ] } > < > > ( ( < { } > ) ) > ) } ) > ] } > )\"\n",
      "\n",
      "< { [ < > ] ( ( ( ( { { } } ) ) ) ) } >, justified by the step-by-step plan and the verification of correctness.\",\n",
      "\n",
      "[ < < [ [ ] ( ) { < > ( [ { } { < > } { } ] ) } [ [ [ ( [ ( ) [ [ { < [ { { } } < { { < ( ) > } } > ] > } ] ] ] ) ] < < [ [ ( < < ( ) > > ) ] ] > > [ ] ] ] ] < ( [ ] ) > { ( ( < { } > ) ) } > > ], [ < < [ [ ] ( ) { < > ( [ { } { < > } { } ] ) } ] [ [ [ ( [ ( ) [ [ { < [ { { } } < { { < ( ) > } } > ] > } ] ] ] ) ] < < [ [ ( < < ( ) > > ) ] ] > > [ ] ] ] ] < ( [ ] ) > { ( ( < { } > ) ) } > > ].\n",
      "\n",
      "< [ { ( ( < ( ( ) ) > ) ) } ] >, < [ { ( ( < ( ( ) ) > ) ) } ] > ) ) } ].\n",
      "\n",
      "{ { [ ( [ { ( { ( [ ( [ ] ) { ( < < [ ] > [ [ ] ] > ) } ] ) [ ] } ) } ] ) ] } { < [ ] > } ( ) }, { { [ ( [ { ( { ( [ ( [ ] ) { ( < < [ ] > [ [ ] ] > ) } ] ) [ ] } ) } ] ) ] } { < [ ] > } ( ).\n",
      "\n",
      "{ [ < [ ] > ] } { ( ) [ < > ] [ [ [ ] ] ] }, { [ < [ ] > ] } { ( ) [ < > ] } [ [ [ ] ] ].\n",
      "\n",
      "[ < [ { ( < ( ( [ < < { } > < < [ ( { < < > > } ) ] > > > ] { } ) ) > ) } ] [ < < { } > ( < < ( ) < ( [ ] ) > > ( ( ) ) > ) > ] > ] < < { } > >, [ < [ { ( < ( ( [ < < { } > < < [ ( { < < > > } ) ] > > > ] { } ) ) > ) } ] [ < < { } > ( < < ( ) < ( [ ] ) > > ( ( ) ) > ) > ] > ] < < {} > > ].\n",
      "\n",
      "{ { [ < > ] } < [ ( [ { } ] ) ] > < [ < > { } ] < ( [ ] ) > < > > }, { { [ < > ] } } < [ ( [ { } ] ) ] ) > < [ < > { } ] } < ( [ ] ) ) > <.\n",
      "\n",
      "( < { } [ ] > ), ( { < [ ] > } ).\n",
      "\n",
      "< [ { { [ [ [ < ( < [ ] { [ ( [ ] ) ] [ ] < > } > ) > ] ( [ ] ) ] ] } } ] >, < [ { { [ [ [ < ( < [ ] { [ ( [ ] ) ] [ ] < > } > ) > ] ( [ ] ) ] ] } } > ] ] } ] >.\n",
      "\n",
      "< { [ ] } > ( ) < [ [ { [ [ ] ] } ] ] ( ) [ < ( [ { } ] ) > ( ( ) ) ] >, < { [] } > () < [ [ { [ [] ] } ] ] () [ < ( [ {} ] ) > ( () ) ].\n",
      "\n",
      "{ ( { { { < ( < ( [ < < > > ] < > ) > ) > } [ ] } } ) }, { { { [  ] } } [ ] }.\n",
      "\n",
      "( ( < < < ( ( ) ) ( [ ] ) > > { [ ] } > ) ), ( ( < < < ( ( ) ) ( [ ] ) > > > > { [ ] } ) ).\n",
      "\n",
      "[ < { { } } > ], [ < { } } > ].\n",
      "\n",
      "( { < [ < > ] > } ), ( { [ < > ] } ).\n",
      "\n",
      "[ ( [ ( [ < ( ) > [ < { } { [ ] } [ ] [ ] > { ( < { < > } > ) } ] ] ) ] ) ], [ ( [ ( [ < ( ) > [ < { } { [ ] } [ ] [ ] > { ( < { < > } > ) } ] ] ) ] ) ] ] > ] } ] ).\n",
      "\n",
      "< { < ( ) > } >, < { < ( ) > > }.\n",
      "\n",
      "( < ( ) ( < [ ] > ( ) < [ [ { } { < > } < > ] ] > ) [ ] > [ ( < { [ ] } [ ] ( { [ < > ] < < > > ( ( < { } > ) ) } ) [ [ [ < < < ( ( ) ) > > > ] < < { [ ] } > > ( [ < > ] ) ] ] > ) ] < > ), None\n",
      "\n",
      "< < ( < > ) > >, < < ( < > ) ) >.\n",
      "\n",
      "( < { < { } > { { ( [ ( { } ) ( ( [ ( [ ( { [ { } ] } ) ] ) ] ) ) [ < [ [ [ [ [ < > ] [ { [ [ { ( ( < [ ] > ) [ ( [ ] ) ] < { [ ] } > ) } ] ] } ] { < ( < > ) > } ] ] ] ] > ] ] ) } } } > ), Yes.\n",
      "\n",
      "< ( ) < > > { < > } { ( ) ( < ( ) > ) } < < { [ ] } > >, justified by the systematic approach used to balance the sequence, ensuring that all parentheses are properly closed.\"\n",
      "\n",
      "< > < { { { [ ( ) ] } } } > ( { [ { ( [ ] { } ) } ] } ), < > < { { { [ ( ) ] } } } > ( { [ { ( [ ] { } ) } ] ) } ] } >.\n",
      "\n",
      "{ < > } [ ( { ( { [ < > ] } ) ( ) } ) ] { [ < > ] } [ [ ( [ ] ) ] ], { < > } [ ( { } ) ] { [ < > ] } [ [ ( [ ] ) ] ].\n",
      "\n",
      "< < { { { < [ ( < > { ( ) } ) ] > } } } > >, < >.\n",
      "\n",
      "< < ( ( [ ] ) ) > >, < < ( ( ) ) > >.\n",
      "\n",
      "< < < { < < > > } > < < { < ( < > ) < > [ [ [ < ( ( ) ) > [ ] [ ] ] ] ] < ( ) > ( ) [ ( [ [ ] ] ) ] > } { } { < [ < { ( { } ) } > ] > } { { } } > > > >, < < < { < < > > } > < < { < ( < > ) < > [ [ [ < ( ( ) ) > [ ] [ ] ] ] ] < ( ) > ( ) [ ( [ [ ] ] ) ] > } { } { < [ < { ( { } ) } > ] > } { { } } > >\n",
      "\n",
      "{ < ( < < < [ < [ [ ] ( ( ) ) ] > < > ] > > > ) > }, { < ( < < < [ < [ [ ] ( ( ) ) ] > < > ] > > > ) > > > > ] > > > >}.\n",
      "\n",
      "{ ( [ ] ) } [ ] ( { < > } ( { } ( { { } } ) ) ( [ ] ) ( ) ( < ( { { ( < { [ ( ) ] } > ) } } [ < [ ( [ { } ] [ < > < < > > ] ) ] > ] { < { } > } < > ) > ) ), { ( [ ] ) } [ ] ( { < > } ( { } ) ) ( [ ] ) ( ) ( < ( { { ( < { [ ( ) ] } > ) } } [ < [ ( [ { } ] [ < > < < > > ] ) ] > ] { < { } > } < > ) > ).\n",
      "\n",
      "[ [ < [ [ ] ] > ] ] { } { ( { ( ( ) ) ( ) { { [ [ ( { < { [ { [ ( < ( ( < < < [ ( ) ] [ ] > > > ) ) > < [ < { < ( ) > } > ] > ) ] } ] } > ( ( ) ) } ) [ ( ) ] ] ( < > ) ] } } } ) } [ ], [ [ < [ [ ] ] > ] ] { } { ( { ( ( ) ) ( ) { { [ [ ( { < { [ { [ ( < ( ( < < < [ ( ) ] [ ] > > > ) ) > < [ < { < ( ) > } > ] > ) ] } ] } > ( ( ) ) } ) [ ( ) ] ] ( < > ) ] } } } ) } ].\n",
      "\n",
      "[ < > ] [ { < { } > { [ ( [ ( ) ] ) ] } } ], [ < > ] [ { < { } > { [ ( [ ( ) ] ) ] } ] ].\n",
      "\n",
      "[ ] { [ [ < { { { } } } > ] ] < [ { } ] > }, [ ] { [ [ < { { { } } } > ] ] < [ { } ] } }.\n",
      "\n",
      "( [ ( { } ) { < ( ( [ ] ) ) > } ] ), ( [ ( { } ) ] { < ( ( [] ) ) > )\n",
      "\n",
      "( { { { { { < < ( { ( { { < < [ < [ [ ] ] > ] [ ] > [ [ ] ] > ( < [ { < > < [ ] > } ] > ) } } ) } ) > > } } } } } ), ( { { { { { < < ( { ( { { < < [ < [ [ ] ] > ] [ ] > [ [ ] ] > ( < [ { < > < [ ] > } ] > ) } } ) } ) > > } } }).\n",
      "\n",
      "( [ [ [ { } ] ] { < [ < [ { } ] > ] > } ] ), ( [ [ [ { } ] ] { < [ < [ { } ] > ] > } ] ] ] ).\n",
      "\n",
      "< ( ( ( [ { } ] ) ) ) >, < ( ( ( [ { } ] ) ) ) ).\n",
      "\n",
      "[ ] ( [ [ { < { { ( < > [ ] ) } } < > > } ] ] { } ), [ ] ( [ [ { < { {  } } < > > } ] ] { }).\n",
      "\n",
      "[ [ < [ ] > ] ] < >, [ [ < > ] ] < >.\n",
      "\n",
      "[ < > [ [ < [ ] > ] ] ], [ < > [ [ < [ ] > ] ] > ].\n",
      "\n",
      "( [ < < > [ [ < > ] ] > ] ( < < < ( [ [ ] ] ) [ ( ( [ { { } } ] ) ) ] > > > ) ), ( [ < < > [ [ < > ] ] > ] ( < < < ( [ [ ] ] ) [ ( ( [ { { } } ] ) ) ] > > ) ) ).\n",
      "\n",
      "{ < [ ] > ( { [ ] } < > ) }, { < [ ] > ( { [ ] } ) < }.\n",
      "\n",
      "( ) ( [ [ ] ] ) ( { [ { < { ( ) < [ [ < > { [ ( < ( < [ { < < ( [ ( ) ] [ ( < ( { [ ] } ) > ) ] [ < > ] ) > [ { ( < > ) } ] > } ] > ) > ) ] } ] ] > { } [ [ ] ] { } } > ( ( < > ) ) } ] } ), The sequence provided in the task is already complete and accurate as given: ( ) ( [ [ ] ] ) ( { [ { < { ( ) < [ [ < > { [ ( < ( < [ { < < ( [ ( ) ] [ ( < ( { [ ] } ) > ) ] [ < > ] ) > [ { ( < > ) } ] > } ] > ) > ) ] } ] ] > { } [ [ ] ] { } } > ( ( < > ) ) } ] }.\n",
      "\n",
      "< [ [ { } ] ] >, < [ [ { } ] ] > ].\n",
      "\n",
      "< { < ( ) ( ( < > < ( ( < < > > ) ) { ( ) } > ) ) { { } } > } >, < { < ( ) ( ( < > ) < ( ( < < > > ) ) { ( ) } > ) ) > { { } } }.\n",
      "\n",
      "[ { [ < [ < { } > ] [ ] > ] } ], `[ { [ < [ < { } > ] [ ] > ] } ] >`.\n",
      "\n",
      "< < [ ( < [ ( { [ ( ) ] [ ( ( { [ ] } ) ) ] } ) ] > ) ] < > > >, < < [ ( < [ ( { [ ( ) ] [ ( ( { [ ] } ) ) ] } ) ] > ) ] < ) ) ).\n",
      "\n",
      "< { { } } ( < [ ] [ { } ] > ) ( { { } { { ( ) } } } ) { < { { [ ] } [ ( ) ] } ( < { [ < > ] } > ) < ( [ ] { ( < < [ ] > > ) [ { [ [ ( ( ) ) ] ] } ] } { } ) > ( { { [ [ ( ) ] ] } } ) > } >, < { { } } ( < [ ] [ { } ] > ) ) ( { { } { { ( ) } } } ) { < { { [ ] } [ ( ) ] } } ( < { [ < > ] } > ) ) < ( [ ] { ( < < [ ] > > ) [ { [ [ ( ( ) ) ] ] } ] } { } ) > ) ( { { [ [ ( ) ] ] } } )\n",
      "\n",
      "{ ( [ { ( < [ { ( ) { } } ] > ) < > } ] ) }, { ( [ { ( < [ { } ] > ) < > } ] ) }.\n",
      "\n",
      "{ < < { ( < > ) } > > }, { < < { ( < > ) } > }.\n",
      "\n",
      "{ { [ [ ] { { < > [ ( ) ( [ ] ) [ ] { < > } ] } } [ { [ ] } ] { ( ) } ] } }, { { [ [ ] { { < > [ ( ) ( [ ] ) [ ] { < > } ] } } [ { [ ] } ] { ( ) } } }.\n",
      "\n",
      "< ( [ { ( < > ) } ] ) > { ( [ ] ) }, < ( [ { ( < > ) } ] ) > { ( [ ] ) } ) ] >.\n",
      "\n",
      "( < < < > > > ), ( < < > > > ).\n",
      "\n",
      "< < [ ( ) ] > >, < [ ( ) ] > >.\n",
      "\n",
      "< ( [ [ [ [ ( ) ] ] { } ] ] ) >, Yes\n",
      "\n",
      "{ [ < [ { < ( ) > } ] > ] }, { [ < [ { < ( ) > } ] > } ] > }.\n",
      "\n",
      "[ { < > < [ ] > [ < { } > ] [ ( { } ) ] [ ] [ ( ) ] ( ) < { { < { { { { { ( { } ) } ( { < > } ) } } } } > } } > } ], [ { < > < [ ] > ] [ < { } > ] [ ( { } ) ] [ ] [ ( ) ] ( ) < { { < { { { { { ( { } ) } } } } } > } } > ].\n",
      "\n",
      "[ ] [ ( [ [ ( ) ] ] ) < < [ ] > > < < > > ], [ ] [ ( [ [ ( ) ] ] ) ] < < [ ] > > < < > >.\n",
      "\n",
      "( { } ) { ( [ { ( ) } ] ( [ ] ) ) }, Yes.\n",
      "\n",
      "[ < > { { { } } { } } ] < { } [ { [ ] } ] >, [ < > { { { } } { } } ] < { } [ { [ ] } ] > } ].\n",
      "\n",
      "{ < > { { ( { } ) } { { ( ( ) ) } } } }, { < > }.\n",
      "\n",
      "{ { ( [ ] ) { ( ) } } }, { { ( [ ] ) { ( ) } ) }.\n",
      "\n",
      "{ ( ( ) ) { < > { ( [ [ { } ] [ ( ) ] ] ) } [ { { { ( < [ ] > ) } { < > } } } ] } }, { ( ( ) ) { < > { ( [ [ { } ] [ ( ) ] ] ) } [ { { { ( < [ ] > ) } { < > } } } ] }.\n",
      "\n",
      "< ( < > ) { < < { ( ( ) { { { < > { } } [ < > ] ( ) } } ( ( ) ) ) < [ { { ( ( < > ) ) } } [ { < { } > } ] ( ) ] > } { [ ] } > > [ ] } > ( [ ] ) [ < { ( ( ( ) ( ) ) ) ( ) } > ] [ < ( ) > ], < ( < > ) { < < { ( ( ) { { { < > { } } [ < > ] ( ) } } ( ( ) ) ) } } { [ ] } > > [ ] } > ( [ ] ) [ < { ( ( ( ) ( ) ) ) ( ) } > ] [ < ( ) > ]\n",
      "\n",
      "( { < { ( ) } > } { ( { [ ] } ) } ), ( { < { ( ) } > } { ( { [ ] } ) } ) } > } ) }.\n",
      "\n",
      "{ [ { [ ] } ] } [ [ { { } } [ < > ] ] ], { [ { [ ] } ] } [ [ { { } } [ < > ] ] >.\"\n",
      "\n",
      "( < < > > < > [ ] [ ] ), ( < < > > < > [ ] [ > ) ].\n",
      "\n",
      "< [ ( { { ( ( ) ) } } ) [ ( [ { } ] ) ] < { { < < < > [ < [ < ( [ ( { ( ( < < < < > > > { ( { { < ( ) > ( ) } } ) } > { } ) ) } ) ] ) > ] > ] > < { } > > } ( ) < { ( ) } > } > ] [ < ( ) > ] >, < [ ( { { ( ( ) ) } } ) [ ( [ { } ] ) ] < { { < < < > [ < [ < ( [ ( { ( ( < < < < > > > { ( { { < ( ) > ( ) } } ) } > { } ) ) } ) ] ) > ] > ] > < { } > > } ( ) < { ( ) } > } > ] [ < ( ) > ] )\n",
      "\n",
      "{ ( { [ ( ) ] } { { [ ] } } { } < > ) } [ < [ ( ) ] > ], { ( { [ ( ) ] } { { [ ] } } { } < > ) } [ < [ ( ) ] ) ] } ] }.\n",
      "\n",
      "{ ( < { < ( ) > } > ) }, { ( < { < ( ) > } > ) } >.\n",
      "\n",
      "[ [ < < { [ ] ( [ [ ( [ [ { [ ] } ] ] ) ( ) ] ] ) ( ) [ ] < [ ( ( ( ) ) ) ] < < < > [ ( ) ] > [ ] > > [ < > ] } > > ] ], [ [ < < { [ ] ( [ [ ( [ [ { [ ] } ] ] ) ( ) ] ] ) ( ) [ ] < [ ( ( ( ) ) ) ] < < < > [ ( ) ] > [ ] > > [ < > ] } > > ] ] ] ].\n",
      "\n",
      "{ { { < { } < ( ( < > ) ) > > } } { [ [ { } ] ] [ ] < < { } > [ ( ) { ( { } ) } ] > ( [ < [ ( ( { } ( ) < > < [ < < { { ( [ { } ] ) } } { } { } > > ] > ( ) ) ) < > ] > ] ) } }, { { { < { } < ( ( < > ) ) > > } } { [ [ { } ] ] [ ] < < { } > [ ( ) { ( { } ) } ] > ( [ < [ ( ( { } ( ) < > < [ < < { { ( [ { } ] ) } } { } { } > > ] > ( ) ) ) < > ] > ] ) } } }.\n",
      "\n",
      "[ [ { { < ( ) > } } ] ], [ [ { { < ( ) > } } ] -> [ [ { { } } ] ] -> [ [ ] ] -> [ ] ]\n",
      "\n",
      "{ { { < > } } < > < { { ( ) } } > { } } { < ( { [ < > ] [ ( ) < ( ) > { < ( ( ( ) ) ) > [ ] } ] < > ( < { } > ) } ) > }, { { { < > } } < > < { { ( ) } } > { } } { < ( { [ < > ] [ ( ) < ( ) > { < ( ( ( ) ) ) > [ ] } ] < > ( < { } > ) } }.\n",
      "\n",
      "[ < > { < [ ] > } ], [ < > { < > } ].\n",
      "\n",
      "[ [ < > ] ], [ [ < ] ] >.\n",
      "\n",
      "{ < ( ) > { < > } ( < > ) < ( ) > [ ] < < < < ( ( ) ) < ( ) > > > > > { } }, { < > { < > } < > < > [ ] < < < < > > > > > }.\n",
      "\n",
      "< < > ( [ ] ) >, < < > ( [ ])> >\n",
      "\n",
      "[ < [ ] > [ ] ], [ < [ ] > [ ] >.\n",
      "\n",
      "< [ ] { < ( ) > } [ ] ( { } ) >, < [ ] { < ( ) > } [ ] ( { } ) > ] >.\n",
      "\n",
      "{ [ < [ < { < { } > } > ( ( < { [ ] } > { { } } ) { } ) ] > ] }, { [ < [ < { < { } > } > ( ( < { [ ] } > { { } } ) { } ) ) ] > ] )\n",
      "\n",
      "{ { [ { < { } > } ( ) ] } }, { { [ { < { } > } ( ) ] ]}}.\n",
      "\n",
      "< [ ] > ( ( ( { { { { ( ( { { } } ) ) } ( { } ) } } { { { [ { ( ) } ( ) ( ( { [ < < > > ] < < > > } ) ) ] ( { } { } ) } } } } ) ) ), None\n",
      "\n",
      "{ < [ < ( < [ ] > ) < > < { } > { < > { } { { < [ < < { } [ { } ] < > > > ] < > > } < [ [ ] ] > } } > ] [ ( ) ] > } [ { ( < ( ( ( [ { } ] ) ) ) > ) } ] [ < ( ) ( ( { ( ) [ { { ( { } ) } } ] } ) ) > ], { < [ < ( < [ ] > ) < > < { } > { < > { } { { < [ < < { } [ { } ] < > > > ] < > > } < [ [ ] ] > } } > ] [ ( ) ] > } [ { ( < ( ( ( [ { } ] ) ) ) > ) } ] [ < ( ) ( ( { ( ) [ { { ( { } ) } } ] } ) ) > ] }.\n",
      "\n",
      "[ < [ ( ( ) < ( ) > ( { { } } [ [ [ < ( [ ] ) ( ) > ] ] ] { { { { { } } } { { } { < [ [ ] ] > } } { } } } ) ) ] > ], yes.\n",
      "\n",
      "< ( ( [ < > { [ { ( ) } ] < { < { } > [ ( < > ) ] } > } [ < > ] ] ) { { ( { ( ( [ ( [ ] ) ] < { } > ) ) { { ( [ [ ] ] ) } [ ( ) ] { { [ ] } } } } ) } ( { } ) } ) >, Yes\n",
      "\n",
      "[ ] { ( ( < { ( ( ( { < > ( ) } ) ) [ { } ] { { ( ) } } ) } < > > ) ) }, [ ] { ( { < > } ( { ( ) } ) ) } [ { } ] { { ( ) } } ) } < > >\n",
      "\n",
      "[ < < > > < ( < < < [ [ [ [ ] ] ] ] > > > ) [ < < [ ] > > ] { [ < > ] < > [ ( ) ] } { [ ] } > ], [ < < > > < ( < < < [ [ [ [ ] ] ] ] > > > ) [ < < [ ] > > ] { [ < > ] < > [ ( ) ] } { [ ] } )\n",
      "\n",
      "( ( ( [ < [ < [ ( [ { < ( < ( ( ) ) { } [ ] > ) > } ] ) { < > } ] [ < [ { ( < < { { [ < { [ [ < > [ ] ] ] } > ] { { } } } } > > ) } ] > { < > { } } ] > ( [ ] ) ] > ] ) ( ) ) ), None\n",
      "\n",
      "( ( ( ( [ [ < [ { { [ ] } } ] > ] ] ( ) ) ) ) ), )\n",
      "\n",
      "< [ { { < ( ) > { < { } > ( < ( ) > { < [ ( { { ( < [ ] > ) } } { ( ( [ [ { } [ ] ] ] ) ) } ) ] > } ) } } } ] >, < [ { { < ( ) > { < { } > ( < ( ) > { < [ ( { { ( < [ ] > ) } } { ( ( [ [ { } [ ] ] ] ) ) } ) ] > } ) } } ] >.\n",
      "\n",
      "[ < [ ( [ < > ] { < > } [ [ ] ] ) ] > ], [ < [ ( [ < > ] { < > } [ [ ] ] ) ) ] } ].\n",
      "\n",
      "( < > [ { [ [ ( [ ] ) [ < ( < > ) { [ ( { ( [ ( ( < ( ( < [ ] { < > } > [ ] ) ) > ) [ { } ] ) ] ) } ) ] } > ] ] ] } [ ] ] ) < [ ] >, ( < > [ { [ [ ( [ ] ) [ < ( < > ) { [ ( { ( [ ( ( < ( ( < [ ] { < > } > [ ] ) ) > ) [ { } ] ) ] ) } ) ] } > ] ] ] } [ ] ] ) < [ > ].\n",
      "\n",
      "{ { < { < > } > } [ ( < < > > ) [ { < ( ) > } ] ] [ ] } < ( { [ < [ { [ ( ) ] } ] > < { ( [ [ [ { } < > { ( < { [ ] } > ( ) ) } ] ] ] ) } > ] } ) >, justified by the systematic approach taken to ensure that each bracket and parenthesis is properly balanced and closed. This involved working through the sequence step-by-step, starting with the innermost brackets and parentheses, and verifying that the overall sequence remains balanced.\"\n",
      "\n",
      "[ < < { } > < > > ( ) ], [ < < { } > < > > ( ) > > ].\n",
      "\n",
      "[ < ( < < > [ ] ( ) > ) > ], [ < ( < < > [ ] ( ) > ) > ) > ].\n",
      "\n",
      "( ( { < { < { < > } > } > } ) ( ) ( { [ ( < ( < < { ( { < > } ) } > > ) > ) { ( { < ( { [ ] } ( ) ) > } < { < { { ( ) } } > { < [ { ( ) } ] > } } > ) } [ [ ( ) ] ] ] } ) ), None\n",
      "\n",
      "[ ] < { < { [ ( ) ] } > } >, [ ].\n",
      "\n",
      "[ < [ ] > ] [ { [ ] } ], [ < [ ] > ] [ { [ ] } ] }.\n",
      "\n",
      "[ < [ [ < > ] ] < ( ) > [ { } ] < [ [ ] ] > ( ) < { < [ ] < < [ [ { [ ] } ] ] > > ( [ ( ( ) ) ] ) > } > [ { { < > } } ] > < ( < { } { [ { [ ] } ] } > ) > ], [ < [ [ < > ] ] < ( ) > [ { } ] < [ [ ] ] > ( ) < { < [ ] < < [ [ { [ ] } ] ] > > ( [ ( ( ) ) ] ) > } > [ { { < > } } ] > < ( < { } { [ { [ ] } ] } > ) >.\n",
      "\n",
      "< < > >, < < > > <.\n",
      "\n",
      "( { ( ) { [ ] { ( ( { < > } ) ) ( < < > < > > < ( ( ) ) > ) } < ( < < < { } > > > ) > } } ), ( { () { [] { ( ( { <> } ) ) ( << > < > > < ( () ) > ) } < ( << < < {} > > > ) > } } ).\n",
      "\n",
      "( ) ( < < { } > > ), ( ) ( < < { } } > > ).\n",
      "\n",
      "[ < [ { { < > [ ( ) ] ( ( ) ) } } ] > ] { { { ( < > ) } } }, [ < [ { { < > [ ( ) ] ( ( ) ) } } ] > ] { { { ( < > ) } } } ].\n",
      "\n",
      "< { ( { } ) < > } >, < { ( { } ) } >.\n",
      "\n",
      "( < [ < ( [ ( ) ] < > < ( { } ) > ) < [ ] > > ] ( ) < [ < > ] > > ), ( < [ < ( [ ( ) ] < > < ( { } ) > ) < [ ] > > ] ( ) < [ < > ] > ).\n",
      "\n",
      "( < < > > [ [ [ { { < { } ( { } ) > } } < { { { } } } < > > ] ] ( ) ] ), ( < < > > [ [ [ { { < { } ( { } ) > } } < { { { } } } < > > ] ] ( ) )\n",
      "\n",
      "( { [ { [ ] { [ [ ] ] } { ( ) } } { [ [ ( [ ] ) ] ] } ] } { < ( ) { [ [ ( ) { [ { { } } ] < > { < ( < { ( < ( [ ] ) > ) } > ) { [ ( < ( ) > ) ] } [ ] { } > } ( ) } ] [ ( ( { < > } ) ) ] ] } > } ), ( { [ { [ ] { [ [ ] ] } { ( ) } } { [ [ ( [ ] ) ] ] } ] } ) { < ( ) { [ [ ( ) { [ { { } } ] < > { < ( < { ( < ( [ ] ) > ) } > ) { [ ( < ( ) > ) ] } [ ] { } > } ( ) } ] [ ( ( { < > } ) ) ] ] } >\n",
      "\n",
      "< < ( [ ] ) > < > >, < < ( [ ] ) > >.\n",
      "\n",
      "[ < [ [ ( ) ( ( { < { < { { } } > } > } ) ) ] ] > ( ) { ( ) } ], [ < [ [ ( ) ( ( { < { < { { } } > } > } ) ) ] ] > > ( ) { ( ) }.\n",
      "\n",
      "[ { ( { [ < ( < [ ( ) ] > ) > ] } ) } ] [ ] [ ( { ( ) } ) ] < { ( ( ( ( ( < > ) ) ) ) ) [ < [ ( < > ) ] > [ [ ] ( ( { } { [ { < [ ] > } ] } < { } > < [ < > ] > [ ] ) ) ] ] } > { [ { ( ) } ] }, [ { ( { [ < ( < [ ( ) ] > ) > ] } ) } ] [ ] [ ( { ( ) } ) ] < { ( ( ( ( ( < > ) ) ) ) ) [ < [ ( < > ) ] > [ [ ] ( ( { } { [ { < [ ] > } ] } < { } > < [ < > ] > [ ] ) ) ] ] } > { [ { ( ) } ] } ].\n",
      "\n",
      "[ < { < [ ( < > ) ( ( [ [ ] ] ) ) ( ( { } ) ) { [ ] } < ( < > ) > ] > } > ], [ < { < [ ( < > ) ( ( [ [ ] ] ) ) ( ( { } ) ) { [ ] } < ( < > ) > ] > } ]\n",
      "\n",
      "( { < { { [ ] } < > [ ] } [ [ [ < [ [ ] ] > ] ] ] > } [ ] { { { { [ { } ] } ( < [ [ [ ] ] ] > ) < < ( { ( [ ] ) [ { { } } ] { [ ] } } ) [ [ { [ [ { ( { } ) [ ( ) { } ] { } } ] ] < > } ] ] > > } } } ), None\n",
      "\n",
      "< ( ) >, < ()\n",
      "\n",
      "{ [ < { < ( ( ) ) > } > ] }, { [ < { < ( ( ) ) > } > ] >}.\n",
      "\n",
      "[ [ ( { { ( { } ) } < < ( [ [ { } < [ { < [ { ( ) ( [ { } ] ) } ] < > > } ] ( < { ( [ ] ) } > ) > ] ( [ < ( ) > ] ) ] ) > > } ) ] ], [ [ ( { { ( { } ) } < < ( [ [ { } < [ { < [ { ( ) ( [ { } ] ) } ] < > > } ] ( < { ( [ ] ) } > ) > ] ( [ < ( ) > ] ) ] ) ) > > } ].\"\n",
      "\n",
      "[ ( < [ [ { [ < [ < ( [ ] ) > ] < > > ( < [ < [ < ( [ ( [ ] ) < ( ) > ] [ ( { ( ) } [ < { { { ( [ ] ) } } } > ] ) ] ) > ] > ] > ) ] } ] ] > ) ( { { { } } } ) ], [ ( < [ [ { [ < [ < ( [ ] ) > ] < > > ( < [ < [ < ( [ ( [ ] ) < ( ) > ] [ ( { ( ) } [ < { { { ( [ ] ) } } } > ] ) ] ) > ] > ] > ) ] } ] ] > ) ( { { { } } } ).\n",
      "\n",
      "< { < { [ < { ( [ ] ) } > { ( { } ) } ( ) ] } > } [ < < > > ] { } [ ] < { } > >, < { < { [ < { ( [ ] ) } > { ( { } ) } ( ) ] } > } [ < < > > ] { } [ ] < { } >.\n",
      "\n",
      "[ < [ ] { { } { < ( { } ) > } } > ] ( { } ), [ < [ ] { { } { < ( { } ) > } } > ] ( {} ) ].\n",
      "\n",
      "{ < < > ( [ [ { ( ( [ ] < > ) ) } ] { [ [ ] { } { [ { < > } ] } ( { { { { } } } } ) ] } ] ) ( ) > }, { < < > ( [ [ { [ ] < > } ] { [ [ ] { } { [ { < > } ] } ( { { { } } } ) ] } ] ) }.\n",
      "\n",
      "[ ( ( { < ( { } ) > } ) ) ], [ ( ( { < ( ) > } ) ].\n",
      "\n",
      "[ { ( { } ) } < < ( ) { { < [ { [ ( ) ] } ] > } } > > ], [ { ( { } ) } < < ( ) { { < [ { [ ( ) ] } ] > } } > >.\n",
      "\n",
      "[ [ [ [ < { ( ) } ( ) > { ( ) } ] { ( ) } ] ] ], [ [ [ [ < { ( ) } ( ) > { ( ) } ] ] ] { } ] ].\n",
      "\n",
      "{ } < { } < > ( ) >, { } < { } < > ().\n",
      "\n",
      "( { } < > { < { ( < > ) } > } ), ( { } < > { < { ( < > ) } > )\n",
      "\n",
      "< [ { { [ { [ { ( ( ) ) } ] } ( < < ( < > ) > { } [ [ [ ( < { } > ) ] ] ] { } > ) ( { } ) { } ] } } ] >, < [ { { [ { [ { ( ( ) ) } ] } ( < < ( < > ) > { } [ [ [ ( < { } > ) ] ] ] { } > ) ( { } ) { } ] } ] >\n",
      "\n",
      "< [ ( [ ( ) [ < [ < > ] < > [ ] > ] < < ( ) ( ) > < { } > > [ < > ] ] ) ] >, < [ ( [ ( ) ] ) ] < [ < > ] < > [ ] > ] < < ( ) ( ) > < { } > > [ < > ] ] > ].\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.492"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperbaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'hyperbaton'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-hyperbaton/bbh-hyperbaton_eval')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the filled JSON for the above reasoning structure:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Step 1: Identify the core rules of adjective order in English grammar\": {\n",
      "        \"Rules\": [\n",
      "            \"Opinion (e.g. wonderful, lovely)\",\n",
      "            \"Size (e.g. big, small)\",\n",
      "            \"Shape (e.g. circular, square)\",\n",
      "            \"Origin (e.g. Pakistani, American)\",\n",
      "            \"Color (e.g. orange, blue)\",\n",
      "            \"Qualifier (e.g. smoking, non-smoking)\"\n",
      "        ],\n",
      "        \"Description\": \"The core rules of adjective order in English grammar dictate that adjectives should be ordered in a specific sequence, starting with opinion, followed by size, shape, origin, color, and qualifier.\"\n",
      "    },\n",
      "    \"Step 2: Break down the adjectives into categories (e.g. size, shape, origin)\": {\n",
      "        \"Adjective categories\": [\n",
      "            {\n",
      "                \"Category\": \"Opinion\",\n",
      "                \"Adjectives\": [\"wonderful\"]\n",
      "            },\n",
      "            {\n",
      "                \"Category\": \"Size\",\n",
      "                \"Adjectives\": [\"big\"]\n",
      "            },\n",
      "            {\n",
      "                \"Category\": \"Shape\",\n",
      "                \"Adjectives\": [\"circular\"]\n",
      "            },\n",
      "            {\n",
      "                \"Category\": \"Origin\",\n",
      "                \"Adjectives\": [\"Pakistani\"]\n",
      "            },\n",
      "            {\n",
      "                \"Category\": \"Color\",\n",
      "                \"Adjectives\": [\"orange\"]\n",
      "            },\n",
      "            {\n",
      "                \"Category\": \"Qualifier\",\n",
      "                \"Adjectives\": [\"smoking\"]\n",
      "            }\n",
      "        ],\n",
      "        \"Description\": \"Breaking down the adjectives into categories helps to identify their correct order.\"\n",
      "    },\n",
      "    \"Step 3: Analyze the adjectives from different perspectives (descriptive, quantitative)\": {\n",
      "        \"Adjective analysis\": [\n",
      "            {\n",
      "                \"Adjective\": \"wonderful\",\n",
      "                \"Perspective\": \"Descriptive\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"big\",\n",
      "                \"Perspective\": \"Quantitative\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"circular\",\n",
      "                \"Perspective\": \"Descriptive\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"Pakistani\",\n",
      "                \"Perspective\": \"Descriptive\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"orange\",\n",
      "                \"Perspective\": \"Descriptive\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"smoking\",\n",
      "                \"Perspective\": \"Descriptive\"\n",
      "            }\n",
      "        ],\n",
      "        \"Description\": \"Analyzing the adjectives from different perspectives helps to understand their functions and correct order.\"\n",
      "    },\n",
      "    \"Step 4: Identify the specific adjectives that need to be ordered correctly in the given sentences\": {\n",
      "        \"Adjectives to order\": [\n",
      "            \"wonderful\",\n",
      "            \"big\",\n",
      "            \"circular\",\n",
      "            \"Pakistani\",\n",
      "            \"orange\",\n",
      "            \"smoking\"\n",
      "        ],\n",
      "        \"Description\": \"The specific adjectives that need to be ordered correctly are identified.\"\n",
      "    },\n",
      "    \"Step 5: Determine if the problem of adjective order is a technical or conceptual one\": {\n",
      "        \"Problem type\": \"Conceptual\",\n",
      "        \"Description\": \"The problem of adjective order is conceptual, as it requires understanding the core rules of adjective order in English grammar.\"\n",
      "    },\n",
      "    \"Step 6: Determine if the problem of adjective order requires innovative solutions or application of established rules\": {\n",
      "        \"Solution type\": \"Application of established rules\",\n",
      "        \"Description\": \"The problem of adjective order requires the application of established rules, as the core rules of adjective order in English grammar are well-established.\"\n",
      "    },\n",
      "    \"Step 7: Analyze the adjectives step by step, considering their type, function, and conventional order\": {\n",
      "        \"Adjective order analysis\": [\n",
      "            {\n",
      "                \"Adjective\": \"wonderful\",\n",
      "                \"Type\": \"Opinion\",\n",
      "                \"Function\": \"Descriptive\",\n",
      "                \"Order\": \"1\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"big\",\n",
      "                \"Type\": \"Size\",\n",
      "                \"Function\": \"Quantitative\",\n",
      "                \"Order\": \"2\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"circular\",\n",
      "                \"Type\": \"Shape\",\n",
      "                \"Function\": \"Descriptive\",\n",
      "                \"Order\": \"3\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"Pakistani\",\n",
      "                \"Type\": \"Origin\",\n",
      "                \"Function\": \"Descriptive\",\n",
      "                \"Order\": \"4\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"orange\",\n",
      "                \"Type\": \"Color\",\n",
      "                \"Function\": \"Descriptive\",\n",
      "                \"Order\": \"5\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"smoking\",\n",
      "                \"Type\": \"Qualifier\",\n",
      "                \"Function\": \"Descriptive\",\n",
      "                \"Order\": \"6\"\n",
      "            }\n",
      "        ],\n",
      "        \"Description\": \"Analyzing the adjectives step by step helps to determine their correct order.\"\n",
      "    },\n",
      "    \"Step 8: Compare the adjective order in the given options with the correct order based on the analysis\": {\n",
      "        \"Option A analysis\": [\n",
      "            {\n",
      "                \"Adjective\": \"wonderful\",\n",
      "                \"Correct order\": \"1\",\n",
      "                \"Actual order\": \"1\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"big\",\n",
      "                \"Correct order\": \"2\",\n",
      "                \"Actual order\": \"2\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"circular\",\n",
      "                \"Correct order\": \"3\",\n",
      "                \"Actual order\": \"3\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"orange\",\n",
      "                \"Correct order\": \"5\",\n",
      "                \"Actual order\": \"4\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"Pakistani\",\n",
      "                \"Correct order\": \"4\",\n",
      "                \"Actual order\": \"5\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"smoking\",\n",
      "                \"Correct order\": \"6\",\n",
      "                \"Actual order\": \"6\"\n",
      "            }\n",
      "        ],\n",
      "        \"Option B analysis\": [\n",
      "            {\n",
      "                \"Adjective\": \"circular\",\n",
      "                \"Correct order\": \"3\",\n",
      "                \"Actual order\": \"1\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"wonderful\",\n",
      "                \"Correct order\": \"1\",\n",
      "                \"Actual order\": \"2\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"smoking\",\n",
      "                \"Correct order\": \"6\",\n",
      "                \"Actual order\": \"3\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"Pakistani\",\n",
      "                \"Correct order\": \"4\",\n",
      "                \"Actual order\": \"4\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"big\",\n",
      "                \"Correct order\": \"2\",\n",
      "                \"Actual order\": \"5\"\n",
      "            },\n",
      "            {\n",
      "                \"Adjective\": \"orange\",\n",
      "                \"Correct order\": \"5\",\n",
      "                \"Actual order\": \"6\"\n",
      "            }\n",
      "        ],\n",
      "        \"Description\": \"Comparing the adjective order in the given options with the correct order helps to identify the correct option.\"\n",
      "    },\n",
      "    \"Step 9: Determine which sentence has the correct adjective order\": {\n",
      "        \"Correct option\": \"Neither A nor B\",\n",
      "        \"Description\": \"Neither option A nor option B has the correct adjective order. The correct order is: wonderful big circular Pakistani orange smoking car.\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is Neither A nor B.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\"'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c286e0b91849bc9886504b0fd28de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A), Neither A nor B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(A), None of the above options. However, based on the instructions to choose one of the options, the closest one would be option (B).\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(A), B.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.908"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logical_deduction_five_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'logical_deduction_five_objects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-logical_deduction_five_objects/bbh-logical_deduction_five_objects_eval')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the filled JSON for the given reasoning structure:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Step 1: Identify the known relationships between the fruits\": {\n",
      "        \"Watermelons vs Cantaloupes\": \"Watermelons are more expensive than cantaloupes\",\n",
      "        \"Mangoes vs Pears\": \"Mangoes are less expensive than pears\",\n",
      "        \"Apples\": \"Apples are the second-cheapest\",\n",
      "        \"Watermelons vs Mangoes\": \"Watermelons are less expensive than mangoes\"\n",
      "    },\n",
      "    \"Step 2: Determine the underlying factors contributing to the price relationships\": {\n",
      "        \"Most expensive fruit\": \"Pears\",\n",
      "        \"Least expensive fruit\": \"Not explicitly stated, but can be deduced from other relationships\",\n",
      "        \"Fruits that are more expensive than others\": \"Watermelons are more expensive than cantaloupes, mangoes are less expensive than pears, watermelons are less expensive than mangoes\"\n",
      "    },\n",
      "    \"Step 3: Analyze the price relationships from different perspectives\": {\n",
      "        \"Assumptions about the relative prices of each fruit\": \"Based on the given relationships, we can assume that pears are the most expensive, and apples are the second-cheapest\",\n",
      "        \"Evidence provided in the statements\": \"The statements provide direct comparisons between some fruits and an absolute position for apples\"\n",
      "    },\n",
      "    \"Step 4: Simplify the price relationships by focusing on the most expensive and least expensive fruits first\": {\n",
      "        \"Most expensive fruit\": \"Pears\",\n",
      "        \"Least expensive fruit\": \"Cantaloupes (deduced from watermelons being more expensive than cantaloupes and apples being the second-cheapest)\",\n",
      "        \"Relationship between the most and least expensive fruits\": \"Pears are more expensive than cantaloupes\"\n",
      "    },\n",
      "    \"Step 5: Break down the price comparisons into smaller parts\": {\n",
      "        \"Comparison 1:\": \"Pears are more expensive than mangoes\",\n",
      "        \"Comparison 2:\": \"Mangoes are more expensive than watermelons\",\n",
      "        \"Comparison 3:\": \"Watermelons are more expensive than cantaloupes\",\n",
      "        \"Comparison 4:\": \"Apples are the second-cheapest, so they must be more expensive than cantaloupes but less expensive than watermelons\"\n",
      "    },\n",
      "    \"Step 6: Create a systematic approach to compare prices and eliminate options\": {\n",
      "        \"Step-by-step plan\": \"Start by placing pears as the most expensive, then use the comparisons to place the other fruits in order\",\n",
      "        \"Possible options to eliminate\": \"Options A, C, D, and E can be eliminated based on the analysis\"\n",
      "    },\n",
      "    \"Step 7: Evaluate the options based on the analysis\": {\n",
      "        \"Option A:\": \"Incorrect, watermelons are not the second-cheapest\",\n",
      "        \"Option B:\": \"Correct, apples are the second-cheapest\",\n",
      "        \"Option C:\": \"Incorrect, mangoes are not the second-cheapest\",\n",
      "        \"Option D:\": \"Incorrect, pears are not the second-cheapest\",\n",
      "        \"Option E:\": \"Incorrect, cantaloupes are not the second-cheapest\"\n",
      "    },\n",
      "    \"Step 8: Determine the correct answer based on the evaluation\": {\n",
      "        \"Correct answer\": \"B\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is B.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\"'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4908af3e224c9bad763fc12de663c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(B), D.\n",
      "\n",
      "(B), D.\n",
      "\n",
      "(E), C.\n",
      "\n",
      "(D), A.\n",
      "\n",
      "(C), one of the given options, and it is option (C).\"\n",
      "\n",
      "(E), C.\n",
      "\n",
      "(E), A.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(E), C.\n",
      "\n",
      "(C), E.\n",
      "\n",
      "(C), D.\n",
      "\n",
      "(E), D.\n",
      "\n",
      "(E), D.\n",
      "\n",
      "(C), E.\n",
      "\n",
      "(D), E.\n",
      "\n",
      "(D), E.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(D), B.\n",
      "\n",
      "(D), consistent with all the given statements, including 'Joe finished last', 'Ana finished second', 'Mya finished second-to-last', and 'Eve finished below Amy'.\"\n",
      "\n",
      "(C), E.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.916"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"]) + 1) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'logical_deduction_seven_objects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/mistral/structured/few_shot_5/bbh/bbh-logical_deduction_seven_objects/bbh-logical_deduction_seven_objects_eval')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'few_shot_examples', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"Step 1: Identify clear positional clues\": {\n",
      "        \"Description\": \"Identify the birds with clear positional clues.\",\n",
      "        \"Action\": \"List the birds with specific positions mentioned in the clues.\",\n",
      "        \"Result\": \"The owl is the second from the right. The cardinal is the fourth from the left. The raven is the second from the left.\"\n",
      "    },\n",
      "    \"Step 2: Place birds with clear positions\": {\n",
      "        \"Description\": \"Place the birds with clear positions on the branch.\",\n",
      "        \"Action\": \"Assign the positions based on the clear clues provided.\",\n",
      "        \"Result\": \"Positions: [Raven, _, Cardinal, _, _, Owl, _]\"\n",
      "    },\n",
      "    \"Step 3: Analyze relative positions\": {\n",
      "        \"Description\": \"Analyze the relative positions of the remaining birds.\",\n",
      "        \"Action\": \"List the relative positions of the birds (e.g., to the left of, to the right of).\",\n",
      "        \"Result\": \"The falcon is to the left of the blue jay. The quail is to the left of the falcon. The robin is to the left of the quail.\"\n",
      "    },\n",
      "    \"Step 4: Place birds based on relative positions\": {\n",
      "        \"Description\": \"Place the remaining birds based on their relative positions.\",\n",
      "        \"Action\": \"Use the relative clues to determine the positions of the remaining birds.\",\n",
      "        \"Result\": \"Positions: [Raven, Robin, Quail, Cardinal, Falcon, Owl, Blue Jay]\"\n",
      "    },\n",
      "    \"Step 5: Verify consistency\": {\n",
      "        \"Description\": \"Verify that all positions are consistent with the given clues.\",\n",
      "        \"Action\": \"Check for any contradictions or inconsistencies in the arrangement.\",\n",
      "        \"Result\": \"All positions are consistent with the given clues.\"\n",
      "    },\n",
      "    \"Step 6: Determine the final arrangement\": {\n",
      "        \"Description\": \"Determine the final arrangement of the birds from left to right.\",\n",
      "        \"Action\": \"List the birds in their final positions.\",\n",
      "        \"Result\": \"Final arrangement: [Raven, Robin, Quail, Cardinal, Falcon, Owl, Blue Jay]\"\n",
      "    },\n",
      "    \"Step 7: Identify the bird second from the right\": {\n",
      "        \"Description\": \"Identify which bird is the second from the right.\",\n",
      "        \"Action\": \"Based on the final arrangement, identify the bird that is second from the right.\",\n",
      "        \"Result\": \"The owl is the second from the right.\"\n",
      "    },\n",
      "    \"Step 8: Select the correct option\": {\n",
      "        \"Description\": \"Select the correct option from the given choices.\",\n",
      "        \"Action\": \"Match the identified bird with the options provided.\",\n",
      "        \"Result\": \"The correct option is (D) The owl is the second from the right.\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is D.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\"'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating...: 250it [00:00, 212995.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(C), D\"\n",
      "\n",
      "(B), (A).\n",
      "\n",
      "(A), B.\"\n",
      "\n",
      "(F), B.\n",
      "\n",
      "(A), F.\n",
      "\n",
      "(C), correct.\"\n",
      "\n",
      "(D), C.\n",
      "\n",
      "(C), G.\n",
      "\n",
      "(E), B.\n",
      "\n",
      "(G), E\"\n",
      "\n",
      "(D), F.\n",
      "\n",
      "(G), C.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(E), B.\n",
      "\n",
      "(G), B.\n",
      "\n",
      "(E), (A) The hawk is the second from the left\"\n",
      "\n",
      "(B), D.\n",
      "\n",
      "(E), G.\n",
      "\n",
      "(F), (D) The black book is the third from the left.\n",
      "\n",
      "(F), A\"\n",
      "\n",
      "(G), (F) The red book is the fourth from the left.\n",
      "\n",
      "(G), C.\n",
      "\n",
      "(C), E.\n",
      "\n",
      "(F), E\"\n",
      "\n",
      "(F), C.\n",
      "\n",
      "(E), C.\n",
      "\n",
      "(B), F.\n",
      "\n",
      "(D), G.\n",
      "\n",
      "(E), C.\n",
      "\n",
      "(G), A.\n",
      "\n",
      "(E), C\"\n",
      "\n",
      "(E), B.\n",
      "\n",
      "(F), D\"\n",
      "\n",
      "(C), F\"\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(E), F\"\n",
      "\n",
      "(G), E.\n",
      "\n",
      "(F), D.\"\n",
      "\n",
      "(A), G.\n",
      "\n",
      "(E), A.\n",
      "\n",
      "(B), F.\n",
      "\n",
      "(E), C.\n",
      "\n",
      "(G), E.\n",
      "\n",
      "(E), G.\n",
      "\n",
      "(B), G.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.816"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logical_deduction_three_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'logical_deduction_three_objects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-logical_deduction_three_objects/bbh-logical_deduction_three_objects_eval')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the filled JSON for the given reasoning structure:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Step 1: Identify the core issue\": \n",
      "        \"The task requires identifying the cheapest fruit among plums, apples, and loquats based on the given statements.\",\n",
      "\n",
      "    \"Step 2: Simplify the given information\": \n",
      "        \"The key points are: (1) The loquats are the cheapest, and (2) The plums are less expensive than the apples.\",\n",
      "\n",
      "    \"Step 3: Analyze the statements from different perspectives\": \n",
      "        \"The first statement directly states that loquats are the cheapest. The second statement compares plums and apples, but since loquats are already established as the cheapest, this comparison only helps to establish the order of the more expensive fruits.\",\n",
      "\n",
      "    \"Step 4: Evaluate the evidence provided\": \n",
      "        \"The evidence directly supports option (C) The loquats are the cheapest. It contradicts options (A) The plums are the cheapest and (B) The apples are the cheapest, as the statements clearly place loquats as the cheapest and plums less expensive than apples but not the cheapest.\",\n",
      "\n",
      "    \"Step 5: Compare the prices of the fruits\": \n",
      "        \"Based on the statements, the order from cheapest to most expensive is: loquats, plums, apples. Therefore, the prices can be compared directly to identify the loquats as the cheapest.\",\n",
      "\n",
      "    \"Step 6: Identify the cheapest fruit\": \n",
      "        \"The loquats are identified as the cheapest fruit based on the comparison.\",\n",
      "\n",
      "    \"Step 7: Verify the answer\": \n",
      "        \"Yes, the identified cheapest fruit (loquats) matches option (C) The loquats are the cheapest.\",\n",
      "\n",
      "    \"Step 8: Draw a conclusion\": \n",
      "        \"The final answer is C.\"\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is C.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\"'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4eaf0985ee3415a85543205bc7ce7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(B), based on logical deduction and consistent with the given information. Therefore, the oranges are the second-most expensive.\"\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(A), consistent with the given statements and logical deductions. The falcon is indeed the leftmost bird.\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'movie_recommendation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/mistral/structured/few_shot_5/bbh/bbh-movie_recommendation/bbh-movie_recommendation_eval')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'few_shot_examples', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"Step 1: Brainstorm Movie Similarities\": {\n",
      "        \"Description\": \"Generate a list of characteristics and themes that make The Shawshank Redemption, Forrest Gump, Dances with Wolves, and Mr. Holland's Opus similar.\",\n",
      "        \"Action\": \"Identify common themes, genres, and emotional impacts.\",\n",
      "        \"Result\": \"Common themes include redemption, personal growth, overcoming adversity, and emotional depth.\"\n",
      "    },\n",
      "    \"Step 2: Explore Different Perspectives\": {\n",
      "        \"Description\": \"Consider different genres, themes, or emotional impacts that could be relevant when comparing the given movies to the options.\",\n",
      "        \"Action\": \"List various perspectives and their relevance to the given movies.\",\n",
      "        \"Result\": \"Perspectives include drama, historical context, character development, and emotional resonance.\"\n",
      "    },\n",
      "    \"Step 3: Critical Movie Analysis\": {\n",
      "        \"Description\": \"Analyze the movies from various angles such as plot, characters, and cinematography.\",\n",
      "        \"Action\": \"Evaluate what makes the given movies unique and assess the options based on these criteria.\",\n",
      "        \"Result\": \"The given movies have strong character arcs, compelling plots, and excellent cinematography.\"\n",
      "    },\n",
      "    \"Step 4: Creative Movie Matching\": {\n",
      "        \"Description\": \"Think outside the box to find unconventional similarities between the given movies and the options.\",\n",
      "        \"Action\": \"Identify unique themes, settings, or storytelling techniques that might link them.\",\n",
      "        \"Result\": \"Unique similarities include themes of justice, societal issues, and personal transformation.\"\n",
      "    },\n",
      "    \"Step 5: Identify Core Movie Theme\": {\n",
      "        \"Description\": \"Determine the central theme or message that ties The Shawshank Redemption, Forrest Gump, Dances with Wolves, and Mr. Holland's Opus together.\",\n",
      "        \"Action\": \"Summarize the core theme of the given movies.\",\n",
      "        \"Result\": \"The core theme is the triumph of the human spirit in the face of adversity.\"\n",
      "    },\n",
      "    \"Step 6: Gather Relevant Movie Data\": {\n",
      "        \"Description\": \"Look for data such as reviews, ratings, and plot summaries that can provide insights into how the options compare to the given movies.\",\n",
      "        \"Action\": \"Collect and analyze relevant data from various sources.\",\n",
      "        \"Result\": \"Reviews and ratings indicate that Philadelphia has similar themes of personal struggle and societal issues.\"\n",
      "    },\n",
      "    \"Step 7: Stakeholder Perspectives\": {\n",
      "        \"Description\": \"Consider the viewpoints of movie enthusiasts, critics, or specific audiences who appreciate the given movies.\",\n",
      "        \"Action\": \"Understand their needs and preferences when evaluating the options.\",\n",
      "        \"Result\": \"Movie enthusiasts appreciate deep emotional stories and strong character development.\"\n",
      "    },\n",
      "    \"Step 8: Movie Expertise Required\": {\n",
      "        \"Description\": \"Assess whether the task requires specific knowledge about movie genres, directors, or cinematic techniques to accurately compare the options to the given movies.\",\n",
      "        \"Action\": \"Determine if specialized knowledge is needed and gather it if necessary.\",\n",
      "        \"Result\": \"Specialized knowledge in drama and character-driven narratives is required.\"\n",
      "    },\n",
      "    \"Step 9: Step-by-Step Movie Comparison\": {\n",
      "        \"Description\": \"Break down the comparison process into systematic steps, evaluating each movie option against the identified similarities one by one.\",\n",
      "        \"Action\": \"Create a comparison matrix or checklist for each movie option.\",\n",
      "        \"Result\": \"Philadelphia matches the criteria of personal struggle, societal issues, and emotional depth.\"\n",
      "    },\n",
      "    \"Step 10: Implement Step-by-Step Plan\": {\n",
      "        \"Description\": \"Create a detailed plan for comparing the movies, including clear steps and explanations for each part of the evaluation process.\",\n",
      "        \"Action\": \"Follow the plan to evaluate each movie option and document the findings.\",\n",
      "        \"Result\": \"Philadelphia is the closest match based on the identified similarities.\"\n",
      "    },\n",
      "    \"Final Step: Conclusion\": {\n",
      "        \"Description\": \"Based on the evaluation, determine which movie option is most similar to the given movies.\",\n",
      "        \"Action\": \"Summarize the findings and make a final recommendation.\",\n",
      "        \"Result\": \"The final answer is C\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is C.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\"'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating...: 250it [00:00, 183381.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A), B.\n",
      "\n",
      "(D), B\"\n",
      "\n",
      "(A), that none of the options (A, B, C, D, E) are similar to the given movies (Goodfellas, Raiders of the Lost Ark, Star Wars Episode IV - A New Hope, The Silence of the Lambs).\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(D), B.\n",
      "\n",
      "(C), A\"\n",
      "\n",
      "(E), D.\n",
      "\n",
      "(C), B\"\n",
      "\n",
      "(A), D\"\n",
      "\n",
      "(A), E.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(B), C.\"\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(B), D.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(D), C.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(A), B.\"\n",
      "\n",
      "(C), D.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(D), E.\n",
      "\n",
      "(D), C.\n",
      "\n",
      "(A), B\"\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(D), A.\n",
      "\n",
      "(A), D\"\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(D), A\"\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(D), E.\n",
      "\n",
      "(A), D.\"\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(A), B\"\n",
      "\n",
      "Monsters, Inc, B\"\n",
      "\n",
      "(D), C.\n",
      "\n",
      "(D), A.\n",
      "\n",
      "(C), None of the options are similar to the given movies.\n",
      "\n",
      "(B), C\"\n",
      "\n",
      "(A), D\"\n",
      "\n",
      "(D), B.\n",
      "\n",
      "(D), B.\n",
      "\n",
      "(D), C.\n",
      "\n",
      "(A), None of the options are similar to the given set of movies.\n",
      "\n",
      "(D), B.\n",
      "\n",
      "(B), D.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(D), A.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(B), D.\n",
      "\n",
      "(C), D.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(C), None of the options are similar to the given movies.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(B), D\"\n",
      "\n",
      "(B), D.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(D), None of the options are closely similar to the given movies in terms of genre, themes, and narrative styles.\n",
      "\n",
      "(C), B\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.716"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multistep_arithmetic_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'multistep_arithmetic_two'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-multistep_arithmetic_two/bbh-multistep_arithmetic_two_eval')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the filled JSON for the above reasoning structure:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Step 1: Identify the core mathematical operations\":\n",
      "        \"The core mathematical operations are multiplication, addition, and subtraction.\",\n",
      "\n",
      "    \"Step 2: Break down the expression into smaller parts\":\n",
      "        \"The expression can be broken down into two smaller parts: (-6 * -1 - 2 + -2) and (9 - 4 + -1 - 7). These expressions inside the parentheses can be evaluated separately.\",\n",
      "\n",
      "    \"Step 3: Evaluate the expressions inside the parentheses\":\n",
      "        {\n",
      "            \"Step 3.1: Evaluate the first set of parentheses\":\n",
      "                \"(-6 * -1 - 2 + -2) = ?\",\n",
      "            \"Step 3.2: Evaluate the second set of parentheses\":\n",
      "                \"(9 - 4 + -1 - 7) = ?\"\n",
      "        },\n",
      "\n",
      "    \"Step 4: Simplify the expressions inside the parentheses\":\n",
      "        {\n",
      "            \"Step 4.1: Simplify the first set of parentheses\":\n",
      "                \"(-6 * -1) = 6, then 6 - 2 = 4, then 4 + -2 = 2\",\n",
      "            \"Step 4.2: Simplify the second set of parentheses\":\n",
      "                \"9 - 4 = 5, then 5 + -1 = 4, then 4 - 7 = -3\"\n",
      "        },\n",
      "\n",
      "    \"Step 5: Combine the simplified expressions\":\n",
      "        \"Add the results of the two simplified expressions: 2 + -3 = -1\",\n",
      "\n",
      "    \"Step 6: Final evaluation\":\n",
      "        \"The final result of the expression is -1.\"\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is -1.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fd50df7d9e4afc98abd5ad46bd6f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5453, -5463.\"\n",
      "\n",
      "-37, 43.\n",
      "\n",
      "250992, 251472.\n",
      "\n",
      "-3850, correct\"\n",
      "\n",
      "-8, 0.\n",
      "\n",
      "343, the result of combining the two expressions, which is 343.\"\n",
      "\n",
      "20, verified to be accurate.\"\n",
      "\n",
      "-20, -18.\n",
      "\n",
      "88, 74.\n",
      "\n",
      "-97, -99.\n",
      "\n",
      "8, verified to be 8\"\n",
      "\n",
      "312, -360.\n",
      "\n",
      "33, reasonable and correct based on the given expression.\"\n",
      "\n",
      "36, -90.\n",
      "\n",
      "630, 1890.\n",
      "\n",
      "48, None\n",
      "\n",
      "42, the result of the multiplication, which is 42.\"\n",
      "\n",
      "5, -17.\n",
      "\n",
      "72, the result of the multiplication, which is 72.\"\n",
      "\n",
      "-113, None\n",
      "\n",
      "107, the result of the subtraction in Step 5.4\"\n",
      "\n",
      "48, 59.\n",
      "\n",
      "101, 59.\n",
      "\n",
      "-43, 85.\n",
      "\n",
      "21, accurate and correct\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"]) + 3) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# penguins_in_a_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'penguins_in_a_table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-penguins_in_a_table/bbh-penguins_in_a_table_eval')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 146\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the filled JSON for the above reasoning structure:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Step 1: Identify the core question being asked\": \"How many penguins are less than 8 years old?\",\n",
      "    \"Step 2: Filter out irrelevant information from the table\": {\n",
      "        \"Columns to focus on\": \"name, age\",\n",
      "        \"Rows to consider\": \"All rows except the header\"\n",
      "    },\n",
      "    \"Step 3: Determine the age threshold for filtering penguins\": \"8 years old\",\n",
      "    \"Step 4: Update the table after deletion\": {\n",
      "        \"Penguin deleted\": \"Bernard\",\n",
      "        \"Remaining penguins\": \"Louis, Vincent, Gwen\"\n",
      "    },\n",
      "    \"Step 5: Go through the table row by row to identify penguins less than 8 years old\": {\n",
      "        \"Penguin 1 (Louis)\": {\n",
      "            \"Age\": \"7\",\n",
      "            \"Meets the age condition\": \"Yes\"\n",
      "        },\n",
      "        \"Penguin 2 (Bernard)\": {\n",
      "            \"Age\": \"5\",\n",
      "            \"Meets the age condition\": \"Yes\",\n",
      "            \"Deleted\": \"Yes\"\n",
      "        },\n",
      "        \"Penguin 3 (Vincent)\": {\n",
      "            \"Age\": \"9\",\n",
      "            \"Meets the age condition\": \"No\"\n",
      "        },\n",
      "        \"Penguin 4 (Gwen)\": {\n",
      "            \"Age\": \"8\",\n",
      "            \"Meets the age condition\": \"No\"\n",
      "        }\n",
      "    },\n",
      "    \"Step 6: Count the penguins meeting the age condition\": \"1 (Louis), note that Bernard was deleted\",\n",
      "    \"Step 7: Choose the final answer based on the count\": \"Since only 1 penguin (Louis) meets the age condition and Bernard was deleted, the answer is option (A) 1.\"\n",
      "}\n",
      "\n",
      "The final answer is A.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\"'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ded1a5d23124754a1e492e7eacd0337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A), None\n",
      "\n",
      "(C), E.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(B), E.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9657534246575342"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reasoning_about_colored_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'reasoning_about_colored_objects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-reasoning_about_colored_objects/bbh-reasoning_about_colored_objects_eval')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the filled JSON for the given task:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Objects on the nightstand\": \"black necklace, green fidget spinner, blue keychain, yellow sheet of paper, red stress ball\",\n",
      "    \"Relevant colors for categorization\": \"yellow and green\",\n",
      "    \"Groups based on yellow and green colors\": {\n",
      "        \"Yellow objects\": \"yellow sheet of paper\",\n",
      "        \"Green objects\": \"green fidget spinner\",\n",
      "        \"Neither yellow nor green objects\": \"black necklace, blue keychain, red stress ball\"\n",
      "    },\n",
      "    \"Specific question being asked\": \"How many objects are neither yellow nor green?\",\n",
      "    \"Available color information\": \"black, green, blue, yellow, red\",\n",
      "    \"Object-by-object analysis\": {\n",
      "        \"Object 1 (black necklace)\": {\n",
      "            \"Color\": \"black\",\n",
      "            \"Is yellow or green\": \"no\",\n",
      "            \"Counted in neither yellow nor green\": \"yes\"\n",
      "        },\n",
      "        \"Object 2 (green fidget spinner)\": {\n",
      "            \"Color\": \"green\",\n",
      "            \"Is yellow or green\": \"yes\",\n",
      "            \"Counted in neither yellow nor green\": \"no\"\n",
      "        },\n",
      "        \"Object 3 (blue keychain)\": {\n",
      "            \"Color\": \"blue\",\n",
      "            \"Is yellow or green\": \"no\",\n",
      "            \"Counted in neither yellow nor green\": \"yes\"\n",
      "        },\n",
      "        \"Object 4 (yellow sheet of paper)\": {\n",
      "            \"Color\": \"yellow\",\n",
      "            \"Is yellow or green\": \"yes\",\n",
      "            \"Counted in neither yellow nor green\": \"no\"\n",
      "        },\n",
      "        \"Object 5 (red stress ball)\": {\n",
      "            \"Color\": \"red\",\n",
      "            \"Is yellow or green\": \"no\",\n",
      "            \"Counted in neither yellow nor green\": \"yes\"\n",
      "        }\n",
      "    },\n",
      "    \"Total count of objects that are neither yellow nor green\": \"3\"\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is C.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\"'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc96d315d7a64458a9542c46f9c2d726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(D), C.\n",
      "\n",
      "(N), D.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(E), (D).\n",
      "\n",
      "(J), F.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(A), E.\n",
      "\n",
      "(H), G.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(G), F.\n",
      "\n",
      "(D), C.\n",
      "\n",
      "(G), F.\n",
      "\n",
      "(E), F.\n",
      "\n",
      "(H), C.\n",
      "\n",
      "(D), J.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(M), G.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(A), G.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.924"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ruin_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'ruin_names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-ruin_names/bbh-ruin_names_eval')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the filled JSON for the above reasoning structure:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Step 1: Identify the core issue or problem\":\n",
      "        \"The core issue is to identify a humorous edit of the artist or movie name 'star wars' by analyzing the options provided.\",\n",
      "    \n",
      "    \"Step 2: Analyze the original name\":\n",
      "        \"The original name 'star wars' has two words that can be targeted for a humorous edit: 'star' and 'wars'. A humorous edit could involve wordplay on either of these words.\",\n",
      "    \n",
      "    \"Step 3: Evaluate each option for wordplay\":\n",
      "        {\n",
      "            \"Option A: stpr wars\": \"There is no apparent wordplay or humorous twist in this option, as it appears to be a typo or a minor variation of the original name.\",\n",
      "            \"Option B: start wars\": \"There is a possible wordplay in this option, as 'start' is a verb that can be associated with the beginning of a conflict, but it's not a strong humorous twist.\",\n",
      "            \"Option C: star warts\": \"There is a clear wordplay in this option, as 'warts' is a common skin condition, and the substitution of 'warts' for 'wars' creates a humorous effect.\",\n",
      "            \"Option D: star warws\": \"There is no apparent wordplay or humorous twist in this option, as it appears to be a typo or a minor variation of the original name.\"\n",
      "        },\n",
      "    \n",
      "    \"Step 4: Consider alternative perspectives\":\n",
      "        \"Alternative perspectives could involve using puns, double meanings, or clever turns of phrase to create a humorous edit. However, the options provided seem to focus on simple word substitutions or variations.\",\n",
      "    \n",
      "    \"Step 5: Generate innovative ideas\":\n",
      "        \"If we think outside the box, a humorous edit of 'star wars' could involve using a phrase like 'star-crossed wars' or 'star-struck wars', which play on common idioms and create a clever connection between the original name and the edit.\",\n",
      "    \n",
      "    \"Step 6: Evaluate the comedic value of each option\":\n",
      "        {\n",
      "            \"Option A: stpr wars\": \"The comedic value of this option is low, as it appears to be a simple typo or variation with no clear humorous intent.\",\n",
      "            \"Option B: start wars\": \"The comedic value of this option is moderate, as it creates a possible wordplay, but it's not a strong or unexpected twist.\",\n",
      "            \"Option C: star warts\": \"The comedic value of this option is high, as it creates a clear and unexpected wordplay that is likely to elicit a humorous response.\",\n",
      "            \"Option D: star warws\": \"The comedic value of this option is low, as it appears to be a simple typo or variation with no clear humorous intent.\"\n",
      "        },\n",
      "    \n",
      "    \"Step 7: Compare and contrast options\":\n",
      "        \"Comparing the options, it's clear that Option C: star warts stands out as the most humorous edit, as it creates a clear and unexpected wordplay that is likely to elicit a humorous response. The other options either lack a clear humorous intent or create a less effective wordplay.\",\n",
      "    \n",
      "    \"Step 8: Make an informed decision\":\n",
      "        \"Based on the analysis, the most humorous edit of 'star wars' is Option C: star warts.\"\n",
      "}\n",
      "\n",
      "The final answer is C.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\",'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8cb31f3c6547fb9e21dc184baeb043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(B), C.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(D), B.\n",
      "\n",
      "rita, sue and bob poo, D.\n",
      "\n",
      "(D), A.\n",
      "\n",
      "(A), C.\n",
      "\n",
      "(B), D.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(D), A.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(D), B.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "dearth, wind, & fire, I.\n",
      "\n",
      "(D), C.\n",
      "\n",
      "(D), C.\n",
      "\n",
      "(B), C.\n",
      "\n",
      "(D), B.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(D), A.\n",
      "\n",
      "(D), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(D), B.\n",
      "\n",
      "(C), A.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.876"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'salient_translation_error_detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/mistral/structured/few_shot_5/bbh/bbh-salient_translation_error_detection/bbh-salient_translation_error_detection_eval')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'few_shot_examples', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"Step 1: Understand the Task\": {\n",
      "        \"Description\": \"Read and understand the task requirements and the types of errors that could occur in the translation.\",\n",
      "        \"Action\": \"Identify the error types: Named Entities, Numerical Values, Modifiers or Adjectives, Negation or Antonyms, Facts, Dropped Content.\"\n",
      "    },\n",
      "    \"Step 2: Analyze the Source Text\": {\n",
      "        \"Description\": \"Carefully read and analyze the source text to understand its content and structure.\",\n",
      "        \"Action\": \"Identify key elements such as named entities, numerical values, modifiers, negations, facts, and significant clauses.\"\n",
      "    },\n",
      "    \"Step 3: Analyze the Translation\": {\n",
      "        \"Description\": \"Carefully read and analyze the translation to understand its content and structure.\",\n",
      "        \"Action\": \"Identify key elements such as named entities, numerical values, modifiers, negations, facts, and significant clauses.\"\n",
      "    },\n",
      "    \"Step 4: Compare Source Text and Translation\": {\n",
      "        \"Description\": \"Compare the source text and the translation side by side.\",\n",
      "        \"Action\": \"Look for discrepancies in named entities, numerical values, modifiers, negations, facts, and significant clauses.\"\n",
      "    },\n",
      "    \"Step 5: Identify Potential Errors\": {\n",
      "        \"Description\": \"Identify any potential errors based on the comparison.\",\n",
      "        \"Action\": \"Note down any discrepancies and categorize them according to the error types.\"\n",
      "    },\n",
      "    \"Step 6: Critical Thinking and Analysis\": {\n",
      "        \"Description\": \"Analyze the translation from different perspectives, question the accuracy of each part, and evaluate the evidence based on the source text.\",\n",
      "        \"Action\": \"Focus on logical reasoning and identifying potential biases or flaws in the translation.\"\n",
      "    },\n",
      "    \"Step 7: Consult Language Rules and Contextual Information\": {\n",
      "        \"Description\": \"Check if there are any relevant language rules or contextual information that can provide insights into the error.\",\n",
      "        \"Action\": \"Use reliable sources to verify language rules and contextual information.\"\n",
      "    },\n",
      "    \"Step 8: Consult Stakeholders or Language Experts\": {\n",
      "        \"Description\": \"Consider consulting stakeholders or language experts who could provide insights into the translation error.\",\n",
      "        \"Action\": \"Gather their perspectives and use them to help identify the error.\"\n",
      "    },\n",
      "    \"Step 9: Determine the Specific Error Type\": {\n",
      "        \"Description\": \"Based on the analysis, determine the specific error type that needs to be identified in the translation.\",\n",
      "        \"Action\": \"Categorize the error as Named Entities, Numerical Values, Modifiers or Adjectives, Negation or Antonyms, Facts, or Dropped Content.\"\n",
      "    },\n",
      "    \"Step 10: Conclude the Error Type\": {\n",
      "        \"Description\": \"Finalize the error type based on the analysis and categorization.\",\n",
      "        \"Action\": \"Select the correct option from the given choices (A, B, C, D, E, F).\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "### Analysis and Conclusion:\n",
      "\n",
      "**Step 1: Understand the Task**\n",
      "- Identify the error types: Named Entities, Numerical Values, Modifiers or Adjectives, Negation or Antonyms, Facts, Dropped Content.\n",
      "\n",
      "**Step 2: Analyze the Source Text**\n",
      "- Source: \"Der Potsdamer Platz ist ein platzartiger Verkehrsknotenpunkt in den Berliner Ortsteilen Mitte und Tiergarten im Bezirk Mitte zwischen der alten Innenstadt im Osten und dem neuen Berliner Westen.\"\n",
      "- Key elements: Named entities (Potsdamer Platz, Mitte, Tiergarten, Bezirk Mitte, alten Innenstadt, neuen Berliner Westen), modifiers (platzartiger, alten, neuen), facts (location and description of Potsdamer Platz).\n",
      "\n",
      "**Step 3: Analyze the Translation**\n",
      "- Translation: \"Potsdamer Platz is a square-like hub in the Berlin districts of Mitte and Tiergarten in the Mitte district between the old city centre in the east and the new west of Berlin.\"\n",
      "- Key elements: Named entities (Potsdamer Platz, Mitte, Tiergarten, Mitte district, old city centre, new west of Berlin), modifiers (square-like, old, new), facts (location and description of Potsdamer Platz).\n",
      "\n",
      "**Step 4: Compare Source Text and Translation**\n",
      "- Discrepancies: The phrase \"im Bezirk Mitte\" is repeated in the translation, which is redundant and not present in the source text.\n",
      "\n",
      "**Step 5: Identify Potential Errors**\n",
      "- The repetition of \"Mitte district\" is a discrepancy that falls under \"Dropped Content\" as it introduces redundancy.\n",
      "\n",
      "**Step 6: Critical Thinking and Analysis**\n",
      "- The repetition of \"Mitte district\" is not accurate and introduces unnecessary redundancy, which is a form of dropped content.\n",
      "\n",
      "**Step 7: Consult Language Rules and Contextual Information**\n",
      "- Verify that the repetition is not a standard translation practice and is indeed an error.\n",
      "\n",
      "**Step 8: Consult Stakeholders or Language Experts**\n",
      "- Gather perspectives from language experts to confirm the error type.\n",
      "\n",
      "**Step 9: Determine the Specific Error Type**\n",
      "- The error is categorized as \"Dropped Content\" due to the redundancy introduced.\n",
      "\n",
      "**Step 10: Conclude the Error Type**\n",
      "- Select the correct option from the given choices (A, B, C, D, E, F).\n",
      "\n",
      "The final answer is E.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\"'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating...: 250it [00:00, 188423.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A), E.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(A), F.\n",
      "\n",
      "(F), D.\"\n",
      "\n",
      "(D), F.\n",
      "\n",
      "(B), D.\n",
      "\n",
      "(A), E.\n",
      "\n",
      "(F), E.\"\n",
      "\n",
      "(F), E.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(A), E.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(E), D. The error is that 'Landkreis Konstanz' is changed to 'district of Constance'.\"\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(C), A.\"\n",
      "\n",
      "(F), D\"\n",
      "\n",
      "(E), B.\n",
      "\n",
      "(A), E.\"\n",
      "\n",
      "(C), F.\n",
      "\n",
      "(C), D.\n",
      "\n",
      "(D), E.\n",
      "\n",
      "(F), A.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(D), F.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(B), E.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(D), E.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(F), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(E), D.\n",
      "\n",
      "(A), E.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(F), E.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(D), E.\n",
      "\n",
      "(F), C.\n",
      "\n",
      "(C), F.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(F), (A) Modifiers or Adjectives.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(B), E.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(B), E.\n",
      "\n",
      "(A), F.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(C), D.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(E), D.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(A), E.\n",
      "\n",
      "(E), D.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(E), D.\n",
      "\n",
      "(A), F.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(A), E.\n",
      "\n",
      "(A), E.\n",
      "\n",
      "(B), D.\n",
      "\n",
      "(C), B.\n",
      "\n",
      "(D), E.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(D), E.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(C), A.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(C), (A) Modifiers or Adjectives.\n",
      "\n",
      "(A), D.\n",
      "\n",
      "(A), F.\n",
      "\n",
      "(C), (A) Modifiers or Adjectives.\n",
      "\n",
      "(F), D.\n",
      "\n",
      "(A), E.\n",
      "\n",
      "(F), D.\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.656"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'snarks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/mistral/structured/few_shot_5/bbh/bbh-snarks/bbh-snarks_eval')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'few_shot_examples', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 178\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"Step 1: Understand the literal meaning of each statement\": {\n",
      "        \"Statement A\": \"Working the printer is too complex for me\",\n",
      "        \"Statement B\": \"Working the microprocessor is too complex for me\"\n",
      "    },\n",
      "    \"Step 2: Analyze the context and tone for any signs of sarcasm\": {\n",
      "        \"Context of Statement A\": \"Statement A is made in a context where the task of using a printer is generally considered simple.\",\n",
      "        \"Context of Statement B\": \"Statement B is made in a context where working with a microprocessor is generally considered complex.\",\n",
      "        \"Tone of Statement A\": \"The tone of Statement A suggests exaggeration and frustration, which could indicate sarcasm.\",\n",
      "        \"Tone of Statement B\": \"The tone of Statement B is more straightforward and could be taken at face value.\"\n",
      "    },\n",
      "    \"Step 3: Identify the core issue or problem that needs to be addressed\": {\n",
      "        \"Core issue of Statement A\": \"The main point of Statement A is the perceived difficulty of using a printer.\",\n",
      "        \"Core issue of Statement B\": \"The main point of Statement B is the actual difficulty of working with a microprocessor.\"\n",
      "    },\n",
      "    \"Step 4: Consider the underlying causes or factors contributing to the problem\": {\n",
      "        \"Underlying causes for Statement A\": \"The context and tone suggest that the speaker is exaggerating the difficulty of a simple task, implying sarcasm.\",\n",
      "        \"Underlying causes for Statement B\": \"The context and tone suggest that the speaker is genuinely expressing difficulty with a complex task.\"\n",
      "    },\n",
      "    \"Step 5: Identify the stakeholders or individuals who are directly affected by the problem\": {\n",
      "        \"Stakeholders for Statement A\": \"The speaker and audience for Statement A are likely to understand the exaggeration and sarcasm.\",\n",
      "        \"Stakeholders for Statement B\": \"The speaker and audience for Statement B are likely to understand the genuine difficulty.\"\n",
      "    },\n",
      "    \"Step 6: Determine if the problem is related to human behavior\": {\n",
      "        \"Human behavior factors for Statement A\": \"The sarcasm in Statement A is influenced by social norms and the speaker's intent to express frustration humorously.\",\n",
      "        \"Human behavior factors for Statement B\": \"The genuine difficulty in Statement B is influenced by the actual complexity of the task.\"\n",
      "    },\n",
      "    \"Step 7: Compare the statements to determine which is more likely to be sarcastic\": {\n",
      "        \"Comparison of sarcasm indicators\": \"Statement A has more indicators of sarcasm, such as exaggeration, context, and tone, compared to Statement B, which is more straightforward.\"\n",
      "    },\n",
      "    \"Step 8: Conclusion\": {\n",
      "        \"Most likely sarcastic statement\": \"Based on the analysis, Statement A is more likely to be sarcastic.\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is A.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\"'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating...: 178it [00:00, 214968.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(B), A.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(B), A\"\n",
      "\n",
      "(B), A\"\n",
      "\n",
      "(A), The statement cannot be determined as sarcastic due to insufficient context.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(B), A\"\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(B), A\"\n",
      "\n",
      "(B), A\"\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(A), B.\n",
      "\n",
      "(B), A.\n",
      "\n",
      "(B), A.\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8707865168539326"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sports_understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'sports_understanding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-sports_understanding/bbh-sports_understanding_eval')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "    \"Assumptions about football and Tyreek Hill\": {\n",
      "        \"Description\": \"List the key assumptions about football and Tyreek Hill that underlie the plausibility of the sentence.\",\n",
      "        \"Assumptions\": [\n",
      "            \"Tyreek Hill is a professional football player.\",\n",
      "            \"Tyreek Hill plays as a wide receiver.\",\n",
      "            \"Tyreek Hill has notable skills, including speed and agility.\",\n",
      "            \"A screen pass is a type of short pass in football, typically caught by a receiver or running back behind the line of scrimmage.\"\n",
      "        ]\n",
      "    },\n",
      "    \"Evidence or context supporting plausibility\": {\n",
      "        \"Description\": \"Identify the kinds of evidence or context that typically support the plausibility of sentences describing sports events.\",\n",
      "        \"Evidence or context\": [\n",
      "            \"Tyreek Hill's past performances and statistics, including his reception and yardage totals.\",\n",
      "            \"Specific football games or situations in which Tyreek Hill has caught screen passes.\",\n",
      "            \"Expert analysis and commentary about Tyreek Hill's abilities and playing style.\"\n",
      "        ]\n",
      "    },\n",
      "    \"Requirement of specific knowledge\": {\n",
      "        \"Description\": \"Determine if evaluating the plausibility of the sentence requires specific knowledge of football or its players.\",\n",
      "        \"Requirement\": \"Yes, evaluating the plausibility of the sentence requires specific knowledge of football, including the rules and strategies of the game, as well as knowledge of Tyreek Hill's skills and past performances.\"\n",
      "    },\n",
      "    \"Underlying factors contributing to plausibility\": {\n",
      "        \"Description\": \"Identify the underlying factors that contribute to the plausibility of Tyreek Hill catching a screen pass.\",\n",
      "        \"Factors\": [\n",
      "            \"Tyreek Hill's position as a wide receiver, which makes it more likely for him to catch a screen pass.\",\n",
      "            \"Tyreek Hill's speed and agility, which make it more likely for him to successfully catch a screen pass.\",\n",
      "            \"The context of the game, including the score, time remaining, and defensive alignment, which can affect the likelihood of a screen pass being called and caught.\"\n",
      "        ]\n",
      "    },\n",
      "    \"Critical analysis and evaluation\": {\n",
      "        \"Description\": \"Analyze the sentence from different perspectives, question assumptions about the sentence's context, and evaluate available information about Tyreek Hill and screen passes.\",\n",
      "        \"Analysis\": \"While Tyreek Hill is a skilled player with a history of making big plays, the sentence 'Tyreek Hill caught the screen pass' is still somewhat generic and lacks specific context. However, given Tyreek Hill's position and skills, it is plausible that he could catch a screen pass in a game.\"\n",
      "    },\n",
      "    \"Core issue affecting plausibility\": {\n",
      "        \"Description\": \"Identify the core issue that affects the plausibility of the sentence.\",\n",
      "        \"Core issue\": \"The core issue affecting plausibility is the lack of specific context about the game situation and the defensive alignment, which can affect the likelihood of a screen pass being called and caught.\"\n",
      "    },\n",
      "    \"Conclusion about plausibility\": {\n",
      "        \"Description\": \"Based on the analysis, determine the plausibility of the sentence.\",\n",
      "        \"Conclusion\": \"The sentence 'Tyreek Hill caught the screen pass' is plausible, given Tyreek Hill's skills and position as a wide receiver. However, the lack of specific context about the game situation and defensive alignment makes it difficult to determine the exact likelihood of the sentence being true.\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is Plausible.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'False',\n",
       "  'False.',\n",
       "  'Highly Unlikely.',\n",
       "  'Highly unlikely.',\n",
       "  'Implausible.',\n",
       "  'Initially implausible but plausible under alternative scenarios.',\n",
       "  'It depends on the context.',\n",
       "  'No',\n",
       "  'No.',\n",
       "  'Not plausible.',\n",
       "  'Plausible',\n",
       "  'Plausible but lacks specificity and context.',\n",
       "  'Plausible but situation-dependent.',\n",
       "  'Plausible but uncertain due to lack of concrete evidence.',\n",
       "  'Plausible.',\n",
       "  'The claim is plausible.',\n",
       "  \"The sentence 'Luke Voit was out at first' is plausible.\",\n",
       "  'The sentence is plausible in certain contexts, but implausible in others.',\n",
       "  'The sentence is somewhat implausible.',\n",
       "  'The sentence is somewhat plausible.',\n",
       "  'The sentence seems implausible.',\n",
       "  'True',\n",
       "  'True.',\n",
       "  'Uncertain',\n",
       "  'Uncertain due to lack of context and evidence.',\n",
       "  'Uncertain, but likely implausible in a literal sense.',\n",
       "  'Uncertain, but possible.',\n",
       "  'Uncertain.',\n",
       "  'Unlikely.',\n",
       "  \"Without additional information or evidence, the claim's plausibility cannot be determined.\",\n",
       "  'Without specific evidence, it is impossible to determine the plausibility of the claim.',\n",
       "  'Yes.'},\n",
       " {'no', 'yes'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset[\"answer_pred\"]), set(dataset[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'False',\n",
       " 'Highly Unlikely',\n",
       " 'Highly unlikely',\n",
       " 'Implausible',\n",
       " 'Initially implausible but plausible under alternative scenarios',\n",
       " 'It depends on the context',\n",
       " 'No',\n",
       " 'Not plausible',\n",
       " 'Plausible',\n",
       " 'Plausible but lacks specificity and context',\n",
       " 'Plausible but situation-dependent',\n",
       " 'Plausible but uncertain due to lack of concrete evidence',\n",
       " 'The claim is plausible',\n",
       " \"The sentence 'Luke Voit was out at first' is plausible\",\n",
       " 'The sentence is plausible in certain contexts, but implausible in others',\n",
       " 'The sentence is somewhat implausible',\n",
       " 'The sentence is somewhat plausible',\n",
       " 'The sentence seems implausible',\n",
       " 'True',\n",
       " 'Uncertain',\n",
       " 'Uncertain due to lack of context and evidence',\n",
       " 'Uncertain, but likely implausible in a literal sense',\n",
       " 'Uncertain, but possible',\n",
       " 'Unlikely',\n",
       " \"Without additional information or evidence, the claim's plausibility cannot be determined\",\n",
       " 'Without specific evidence, it is impossible to determine the plausibility of the claim',\n",
       " 'Yes'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_fn(instance):\n",
    "    return {\n",
    "        \"answer_pred\": instance[\"answer_pred\"].translate(str.maketrans(\"\", \"\", '.\"')),\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(map_fn)\n",
    "\n",
    "set(dataset[\"answer_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Initially implausible but plausible under alternative scenarios',\n",
       " 'It depends on the context',\n",
       " 'Plausible but lacks specificity and context',\n",
       " 'Plausible but situation-dependent',\n",
       " 'Plausible but uncertain due to lack of concrete evidence',\n",
       " 'The sentence is plausible in certain contexts, but implausible in others',\n",
       " 'The sentence is somewhat implausible',\n",
       " 'The sentence is somewhat plausible',\n",
       " 'Uncertain',\n",
       " 'Uncertain due to lack of context and evidence',\n",
       " 'Uncertain, but likely implausible in a literal sense',\n",
       " 'Uncertain, but possible',\n",
       " \"Without additional information or evidence, the claim's plausibility cannot be determined\",\n",
       " 'Without specific evidence, it is impossible to determine the plausibility of the claim',\n",
       " 'no',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plausible (Yes)\n",
    "plausible_yes = [\n",
    "    \"Plausible\",\n",
    "    \"The claim is plausible\",\n",
    "    \"The sentence 'Luke Voit was out at first' is plausible\",\n",
    "    \"Yes\",\n",
    "    \"True\",\n",
    "]\n",
    "\n",
    "# Implausible (No)\n",
    "implausible_no = [\n",
    "    \"False\",\n",
    "    \"Highly Unlikely\",\n",
    "    \"Highly unlikely\",\n",
    "    \"Implausible\",\n",
    "    \"No\",\n",
    "    \"Not plausible\",\n",
    "    \"The sentence seems implausible\",\n",
    "    \"Unlikely\",\n",
    "]\n",
    "\n",
    "# Indeterminate\n",
    "indeterminate = [\n",
    "    \"Initially implausible but plausible under alternative scenarios\",\n",
    "    \"The sentence is somewhat implausible\",\n",
    "    \"It depends on the context\",\n",
    "    \"The sentence is plausible in certain contexts, but implausible in others\",\n",
    "    \"The sentence is somewhat plausible\",\n",
    "    \"Uncertain\",\n",
    "    \"Uncertain due to lack of context and evidence\",\n",
    "    \"Uncertain, but likely implausible in a literal sense\",\n",
    "    \"Uncertain, but possible\",\n",
    "    \"Without additional information or evidence, the claim's plausibility cannot be determined\",\n",
    "    \"Without specific evidence, it is impossible to determine the plausibility of the claim\",\n",
    "    \"Plausible but lacks specificity and context\",\n",
    "    \"Plausible but situation-dependent\",\n",
    "    \"Plausible but uncertain due to lack of concrete evidence\",\n",
    "]\n",
    "\n",
    "\n",
    "def map_fn(ins):\n",
    "    for yes in plausible_yes:\n",
    "        if yes == ins[\"answer_pred\"]:\n",
    "            return {\n",
    "                \"answer_pred\": \"yes\"\n",
    "            }\n",
    "\n",
    "    for no in implausible_no:\n",
    "        if no == ins[\"answer_pred\"]:\n",
    "            return {\n",
    "                \"answer_pred\": \"no\"\n",
    "            }\n",
    "    return {\n",
    "        \"answer_pred\": ins[\"answer_pred\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(map_fn)\n",
    "set(dataset[\"answer_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0542158ceb004933a82999939266bbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uncertain\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The sentence is somewhat implausible\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The sentence is somewhat plausible\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uncertain due to lack of context and evidence\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uncertain\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Plausible but situation-dependent\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The sentence is plausible in certain contexts, but implausible in others\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uncertain, but likely implausible in a literal sense\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Plausible but lacks specificity and context\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uncertain\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "It depends on the context\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Initially implausible but plausible under alternative scenarios\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uncertain, but possible\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The sentence is somewhat implausible\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uncertain\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uncertain\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Without specific evidence, it is impossible to determine the plausibility of the claim\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Without additional information or evidence, the claim's plausibility cannot be determined\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Plausible but uncertain due to lack of concrete evidence\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uncertain\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uncertain\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Uncertain\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "yes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "no\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The sentence is somewhat plausible\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.708"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# temporal_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'temporal_sequences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-temporal_sequences/bbh-temporal_sequences_eval')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the filled JSON for the above reasoning structure:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Step 1: Identify given time intervals\":\n",
      "    {\n",
      "        \"Tiffany woke up\": \"6am\",\n",
      "        \"Reading at the library\": \"6am-9am\",\n",
      "        \"Driving to the water park\": \"9am-10am\",\n",
      "        \"Buying a phone at the electronics store\": \"12pm-6pm\",\n",
      "        \"Working out at the gym\": \"6pm-10pm\",\n",
      "        \"Market closure\": \"after 10pm\"\n",
      "    },\n",
      "    \"Step 2: Eliminate irrelevant time periods\":\n",
      "    {\n",
      "        \"Time periods to eliminate\": [\"6am-9am\", \"9am-10am\", \"12pm-6pm\", \"6pm-10pm\"],\n",
      "        \"Reasoning for elimination\": [\"Tiffany was reading\", \"Tiffany was driving\", \"Tiffany was buying a phone\", \"Tiffany was working out\"]\n",
      "    },\n",
      "    \"Step 3: Break down Tiffany's daily activities into separate time blocks\":\n",
      "    {\n",
      "        \"Time blocks\": [\n",
      "            {\"start\": \"6am\", \"end\": \"9am\", \"activity\": \"reading\"},\n",
      "            {\"start\": \"9am\", \"end\": \"10am\", \"activity\": \"driving\"},\n",
      "            {\"start\": \"10am\", \"end\": \"12pm\", \"activity\": \"\"},\n",
      "            {\"start\": \"12pm\", \"end\": \"6pm\", \"activity\": \"buying a phone\"},\n",
      "            {\"start\": \"6pm\", \"end\": \"10pm\", \"activity\": \"working out\"}\n",
      "        ]\n",
      "    },\n",
      "    \"Step 4: Identify gaps in Tiffany's activities where she could have gone to the market\":\n",
      "    {\n",
      "        \"Potential time intervals for market visit\": [\"10am-12pm\"]\n",
      "    },\n",
      "    \"Step 5: Analyze each time interval step by step\":\n",
      "    {\n",
      "        \"Time interval analysis\": [\"Since the market was closed after 10pm, Tiffany must have gone to the market before 10pm. The only available time slot is 10am-12pm.\"]\n",
      "    },\n",
      "    \"Step 6: Create a timeline of Tiffany's activities and identify the most likely time window for her market visit\":\n",
      "    {\n",
      "        \"Timeline\": [\n",
      "            {\"start\": \"6am\", \"end\": \"9am\", \"activity\": \"reading\"},\n",
      "            {\"start\": \"9am\", \"end\": \"10am\", \"activity\": \"driving\"},\n",
      "            {\"start\": \"10am\", \"end\": \"12pm\", \"activity\": \"market visit\"},\n",
      "            {\"start\": \"12pm\", \"end\": \"6pm\", \"activity\": \"buying a phone\"},\n",
      "            {\"start\": \"6pm\", \"end\": \"10pm\", \"activity\": \"working out\"}\n",
      "        ],\n",
      "        \"Most likely time window\": [\"10am-12pm\"]\n",
      "    },\n",
      "    \"Step 7: Determine the final answer\":\n",
      "    {\n",
      "        \"Final answer\": [\"B\"]\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is B.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh = lambda y_i, y_pred_i: y_pred_i and y_i.translate(\n",
    "    str.maketrans(\"\", \"\", \"()\")\n",
    ") == y_pred_i.translate(str.maketrans(\"\", \"\", '.()\"'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4702675e17d34eaaa6cb842a4030a4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(C), A.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# web_of_lies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'web_of_lies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-web_of_lies/bbh-web_of_lies_eval')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "    \"Step 1: Identify the statements and their relationships\": {\n",
      "        \"Raymond's statement\": \"Raymond tells the truth\",\n",
      "        \"Sal's statement about Raymond\": \"Sal says Raymond lies\",\n",
      "        \"Alexis's statement about Sal\": \"Alexis says Sal lies\",\n",
      "        \"Helene's statement about Alexis\": \"Helene says Alexis lies\",\n",
      "        \"Elanor's statement about Helene\": \"Elanor says Helene lies\"\n",
      "    },\n",
      "    \"Step 2: Determine the truth value of Raymond's statement\": {\n",
      "        \"Assume Raymond tells the truth\": \"If Raymond tells the truth, then his statement is true\",\n",
      "        \"Assume Raymond lies\": \"If Raymond lies, then his statement is false, which is a contradiction\"\n",
      "    },\n",
      "    \"Step 3: Analyze Sal's statement about Raymond\": {\n",
      "        \"If Raymond tells the truth, then Sal's statement is\": \"false, because Sal says Raymond lies\",\n",
      "        \"If Raymond lies, then Sal's statement is\": \"This case is not possible due to the contradiction in Step 2\"\n",
      "    },\n",
      "    \"Step 4: Evaluate Alexis's statement about Sal\": {\n",
      "        \"If Sal tells the truth, then Alexis's statement is\": \"Not applicable, since Sal's statement is false\",\n",
      "        \"If Sal lies, then Alexis's statement is\": \"true, because Alexis says Sal lies\"\n",
      "    },\n",
      "    \"Step 5: Examine Helene's statement about Alexis\": {\n",
      "        \"If Alexis tells the truth, then Helene's statement is\": \"false, because Helene says Alexis lies\",\n",
      "        \"If Alexis lies, then Helene's statement is\": \"Not applicable, since Alexis's statement is true\"\n",
      "    },\n",
      "    \"Step 6: Assess Elanor's statement about Helene\": {\n",
      "        \"If Helene tells the truth, then Elanor's statement is\": \"Not applicable, since Helene's statement is false\",\n",
      "        \"If Helene lies, then Elanor's statement is\": \"true, because Elanor says Helene lies\"\n",
      "    },\n",
      "    \"Step 7: Identify potential contradictions and patterns\": {\n",
      "        \"Contradictions between statements\": \"If Raymond lies, his statement is false, which is a contradiction\",\n",
      "        \"Patterns or relationships between statements\": \"Each person's statement is the opposite of the previous person's statement\"\n",
      "    },\n",
      "    \"Step 8: Draw conclusions about Elanor's statement\": {\n",
      "        \"Based on the analysis, Elanor's statement is likely\": \"true\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is True.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Cannot be determined.',\n",
       "  'False.',\n",
       "  'Inga tells the truth if and only if Alexis lies',\n",
       "  'No',\n",
       "  'No.',\n",
       "  'T.',\n",
       "  'True',\n",
       "  'True.',\n",
       "  'True.\"',\n",
       "  'Yes',\n",
       "  'Yes.',\n",
       "  'false.',\n",
       "  'no.',\n",
       "  'yes.'},\n",
       " {'No', 'Yes'})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset[\"answer_pred\"]), set(dataset[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a16b5d475449ee8b0f491d42c3847b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Cannot be determined',\n",
       " 'False',\n",
       " 'Inga tells the truth if and only if Alexis lies',\n",
       " 'No',\n",
       " 'T',\n",
       " 'True',\n",
       " 'Yes',\n",
       " 'false',\n",
       " 'no',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_fn(instance):\n",
    "    return {\n",
    "        \"answer_pred\": instance[\"answer_pred\"].translate(str.maketrans(\"\", \"\", '.\"*')),\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(map_fn)\n",
    "\n",
    "set(dataset[\"answer_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d529027b0fc4b329b26fa91ef026e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Cannot be determined',\n",
       " 'Inga tells the truth if and only if Alexis lies',\n",
       " 'No',\n",
       " 'Yes'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truth (Yes)\n",
    "truth_yes = [\n",
    "    'T',\n",
    "    'True',\n",
    "    'Yes',\n",
    "    'yes'\n",
    "]\n",
    "\n",
    "# False (No)\n",
    "false_no = [\n",
    "    'False',\n",
    "    'No',\n",
    "    'false',\n",
    "    'no',\n",
    "]\n",
    "\n",
    "indeterminate = [\n",
    "    'Cannot be determined',\n",
    "    'Inga tells the truth if and only if Alexis lies',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def map_fn(ins):\n",
    "    for yes in truth_yes:\n",
    "        if yes == ins[\"answer_pred\"]:\n",
    "            return {\n",
    "                \"answer_pred\": \"Yes\"\n",
    "            }\n",
    "\n",
    "    for no in false_no:\n",
    "        if no == ins[\"answer_pred\"]:\n",
    "            return {\n",
    "                \"answer_pred\": \"No\"\n",
    "            }\n",
    "    return {\n",
    "        \"answer_pred\": ins[\"answer_pred\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(map_fn)\n",
    "set(dataset[\"answer_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7109e04e7c343f0b83829ba58704eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n",
      "Yes, No\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n",
      "Yes, No\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Inga tells the truth if and only if Alexis lies\n",
      "\n",
      "Yes, No\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Cannot be determined\n",
      "\n",
      "Yes, No\n",
      "\n",
      "Yes, No\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n",
      "Yes, No\n",
      "\n",
      "No, Yes\n",
      "\n",
      "No, Yes\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.888"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"])) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word_sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'word_sorting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/dev/self-discover/evals/logs/phased_self_discover/llama/structured/few_shot_0/bbh/bbh-word_sorting/bbh-word_sorting_eval')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = here(os.path.join(base_path, f\"bbh-{subset}\", f\"bbh-{subset}_eval\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the filled JSON for the above reasoning structure:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Step 1: Identify the list of words to be sorted\": {\n",
      "        \"List of words\": \"slurp, raytheon, gloucester\"\n",
      "    },\n",
      "    \"Step 2: Separate the list into individual words\": {\n",
      "        \"Word 1\": \"slurp\",\n",
      "        \"Word 2\": \"raytheon\",\n",
      "        \"Word 3\": \"gloucester\"\n",
      "    },\n",
      "    \"Step 3: Identify the first letter of each word\": {\n",
      "        \"Word 1 first letter\": \"s\",\n",
      "        \"Word 2 first letter\": \"r\",\n",
      "        \"Word 3 first letter\": \"g\"\n",
      "    },\n",
      "    \"Step 4: Compare the first letters of each word to determine the alphabetical order\": {\n",
      "        \"Comparison of Word 1 and Word 2 first letters\": \"s comes after r\",\n",
      "        \"Comparison of Word 2 and Word 3 first letters\": \"r comes after g\",\n",
      "        \"Comparison of Word 1 and Word 3 first letters\": \"s comes after g\"\n",
      "    },\n",
      "    \"Step 5: If the first letters are the same, compare the subsequent letters of each word\": {\n",
      "        \"Comparison of Word 1 and Word 2 subsequent letters\": \"not necessary\",\n",
      "        \"Comparison of Word 2 and Word 3 subsequent letters\": \"not necessary\",\n",
      "        \"Comparison of Word 1 and Word 3 subsequent letters\": \"not necessary\"\n",
      "    },\n",
      "    \"Step 6: Determine the alphabetical order of the words based on the comparisons\": {\n",
      "        \"Alphabetical order of the words\": \"gloucester, raytheon, slurp\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "The final answer is gloucester, raytheon, slurp.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_pred_list = [x.translate(str.maketrans(\"\", \"\", \".'\")) for x in dataset[\"answer_pred\"] if x and '[' in x]\n",
    "len(answer_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chlorate',\n",
       " 'glidden',\n",
       " 'incentive',\n",
       " 'judicatory',\n",
       " 'lavoisier',\n",
       " 'manatee',\n",
       " 'spurt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_pred_list[0].translate(str.maketrans(\"\", \"\", \"'\\\"[]\")).split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " 'Yes.',\n",
       " '[',\n",
       " '[\"Amethyst\", \"Bathos\", \"Dormouse\", \"Obtuse\", \"Resignation\", \"Walt\"].',\n",
       " '[\"Dateline\", \"Household\", \"Jill\", \"Langmuir\", \"Pipette\"].',\n",
       " '[\"abc\", \"ada\", \"austere\", \"blend\", \"cankerworm\", \"falcon\", \"flamboyant\", \"gag\", \"grecian\", \"hanukkah\", \"indicate\", \"kruger\", \"lobster\", \"militia\", \"nobody\", \"pierson\", \"quad\", \"right\", \"ron\", \"wildcat\"].',\n",
       " '[\"aberdeen\", \"analogue\", \"deciduous\", \"easel\", \"sprightly\", \"swaziland\"].',\n",
       " '[\"abner\", \"abramson\", \"amity\", \"automate\", \"exquisite\", \"fruitful\", \"gurgle\", \"none\", \"shampoo\", \"shorten\", \"waterproof\"].',\n",
       " '[\"above\", \"big\", \"broken\", \"coexist\", \"dominate\", \"irk\", \"olive\", \"prometheus\", \"screw\", \"thirdhand\"].',\n",
       " '[\"abramson\", \"bangui\", \"carlisle\", \"cavalier\", \"contextual\", \"dustbin\", \"emacs\", \"implementor\", \"islamabad\", \"magistrate\", \"nudge\", \"picnicking\", \"railway\", \"refractory\", \"silvery\", \"waite\"].',\n",
       " '[\"accelerate\", \"bauer\", \"county\", \"nail\", \"nominee\", \"o\\'connell\", \"phony\", \"poole\", \"putnam\", \"quantify\", \"raisin\", \"venice\"].',\n",
       " '[\"accept\", \"alpenstock\", \"angus\", \"castigate\", \"chromium\", \"concision\", \"doge\", \"drool\", \"elizabethan\", \"jutish\", \"marshmallow\", \"ocean\", \"octennial\", \"prize\", \"resistive\", \"stonewort\", \"vociferous\"].',\n",
       " '[\"accessory\", \"admiration\", \"allusion\", \"bandgap\", \"bruckner\", \"cruise\", \"fungus\", \"gambit\", \"heron\", \"maidenhair\", \"postprocessor\", \"proscenium\", \"technion\"].',\n",
       " '[\"accrue\", \"archipelago\", \"biplane\", \"breezy\", \"canada\", \"conspiracy\", \"constructor\", \"dobbins\", \"germinal\", \"hamburger\", \"insubstantial\", \"laramie\", \"lost\", \"malleable\", \"nutrient\", \"peloponnese\", \"ted\", \"thigh\"].',\n",
       " '[\"admixture\", \"catwalk\", \"chateaux\", \"coordinate\", \"equine\", \"higgins\", \"irremediable\", \"malthusian\", \"offertory\", \"panamanian\", \"pecos\", \"reluctant\", \"shelve\", \"suction\", \"tunis\"].',\n",
       " '[\"adonis\", \"birdseed\", \"citizen\", \"contaminant\", \"convair\", \"extensive\", \"fateful\", \"frighten\", \"judaica\", \"scrubby\", \"soothe\", \"southeastern\", \"stormy\", \"suppose\", \"trillion\", \"trundle\"].',\n",
       " '[\"adsorption\", \"align\", \"anastasia\", \"anastomotic\", \"apache\", \"award\", \"bobbin\", \"burrow\", \"calumny\", \"epaulet\", \"execrable\", \"hostelry\", \"hun\", \"macedon\", \"omnipotent\", \"putty\", \"roughshod\", \"smooth\", \"spontaneity\"].',\n",
       " '[\"aeneas\", \"colombo\", \"foothold\", \"fox\", \"garry\", \"glycerine\", \"inviolate\", \"lucre\", \"magnanimity\", \"nevada\", \"notoriety\", \"plebiscite\", \"pompey\", \"quagmire\", \"satanic\", \"scription\", \"softball\", \"spleenwort\", \"tennyson\", \"type\"].',\n",
       " '[\"aerodynamic\", \"botanist\", \"giacomo\", \"habitation\", \"jimmy\", \"nebulous\", \"offset\", \"padre\", \"panicking\", \"roosevelt\", \"schoolmate\", \"suburbia\", \"vector\", \"wv\"].',\n",
       " '[\"affirmative\", \"airframe\", \"arcing\", \"ballroom\", \"bassoon\", \"benefit\", \"buggy\", \"coupon\", \"decide\", \"dodge\", \"hypothermia\", \"intrepid\", \"junior\", \"ladle\", \"nineveh\", \"prorogue\", \"schmitt\", \"shagging\", \"sparse\", \"ulcerate\"].',\n",
       " '[\"affluent\", \"cheshire\", \"covalent\", \"diagnostician\", \"divisive\", \"epsilon\", \"folklore\", \"gideon\", \"gothic\", \"grover\", \"horowitz\", \"julio\", \"peanut\", \"quadrature\", \"salient\", \"spiderwort\", \"spiritual\"].',\n",
       " '[\"afloat\", \"apostasy\", \"bechtel\", \"chattel\", \"conner\", \"ferment\", \"grosbeak\", \"hendrickson\", \"indonesia\", \"jacm\", \"lanthanide\", \"melancholy\", \"quark\", \"scavenge\", \"strove\", \"vibrate\"].',\n",
       " '[\"afro\", \"blackbird\", \"blame\", \"calyx\", \"elgin\", \"emphases\", \"implacable\", \"jura\", \"mayapple\", \"perquisite\", \"whit\"].',\n",
       " '[\"agamemnon\", \"clench\", \"depreciate\", \"eject\", \"forum\", \"frame\", \"herbivorous\", \"lien\", \"marcello\", \"numbly\", \"search\", \"sprout\", \"unary\", \"zaire\"].',\n",
       " '[\"agglomerate\", \"ballast\", \"dollop\", \"erosible\", \"expiry\", \"extensor\", \"gazpacho\", \"indiscreet\", \"manuel\", \"ogle\", \"oilcloth\", \"spaniard\"].',\n",
       " '[\"aggression\", \"arachne\", \"asplenium\", \"bystander\", \"definite\", \"gneiss\", \"lengthy\", \"sanford\", \"southeast\", \"translate\"].',\n",
       " '[\"airlift\", \"butch\", \"cone\", \"homeowner\", \"inanimate\", \"incurring\", \"logarithm\", \"lumber\", \"maladapt\", \"micron\", \"newman\", \"profuse\", \"robertson\", \"sammy\", \"souvenir\", \"uganda\", \"wilcox\"].',\n",
       " '[\"aitken\", \"barycentric\", \"detest\", \"downey\", \"kajar\", \"nat\", \"solvate\", \"usable\", \"vision\"].',\n",
       " '[\"alkali\", \"breach\", \"buckle\", \"falsetto\", \"hyperboloid\", \"liquidate\", \"mirth\", \"nagasaki\", \"parmesan\"].',\n",
       " '[\"allocable\", \"bertram\", \"boutique\", \"champlain\", \"crunchy\", \"dissipate\", \"facto\", \"highlight\", \"hydrology\", \"judaism\", \"labile\", \"necessity\", \"often\", \"phenol\", \"silage\", \"vale\"].',\n",
       " '[\"allocate\", \"ann\", \"bishopric\", \"blake\", \"carbondale\", \"casual\", \"confirmatory\", \"crinkle\", \"degum\", \"elliot\", \"expatriate\", \"hangable\", \"neal\", \"orthodontist\", \"shenandoah\", \"soybean\", \"telegraph\", \"tuxedo\", \"unipolar\"].',\n",
       " '[\"allot\", \"chauncey\", \"clergymen\", \"coachmen\", \"coddington\", \"companion\", \"embark\", \"fatten\", \"gazpacho\", \"granular\", \"hobble\", \"murk\", \"muslim\", \"niggle\", \"pristine\", \"pvc\", \"singlet\", \"threefold\", \"too\", \"yeats\"].',\n",
       " '[\"allstate\", \"dose\", \"dyad\", \"multitudinous\", \"plural\", \"powderpuff\", \"stalin\"].',\n",
       " '[\"allyn\", \"carbonaceous\", \"cetacean\", \"investigatory\", \"johann\", \"majorca\", \"paradigmatic\", \"pathogenic\", \"pray\", \"supersede\", \"tung\"].',\n",
       " '[\"alterate\", \"aseptic\", \"cayenne\", \"chandigarh\", \"debauch\", \"declassify\", \"dingy\", \"equanimity\", \"excursion\", \"foamflower\", \"groupoid\", \"inclement\", \"kruger\", \"lawful\", \"october\", \"only\", \"scorch\"].',\n",
       " '[\"altercate\", \"cornerstone\", \"courtroom\", \"dusenberg\", \"foraminifera\", \"gossamer\", \"insist\", \"jive\", \"promulgate\", \"raft\", \"sal\", \"sophocles\", \"syllabus\", \"wrongdoer\"].',\n",
       " '[\"alternate\", \"boone\", \"chalcedony\", \"charity\", \"genteel\", \"million\", \"olden\", \"satin\", \"sinai\"].',\n",
       " '[\"ami\", \"bituminous\", \"decadent\", \"exeter\", \"knickerbocker\"].',\n",
       " '[\"amicable\", \"browne\", \"calumny\", \"coo\", \"deerstalker\", \"extreme\", \"henchman\", \"histology\", \"indoeuropean\", \"paginate\", \"pelvis\", \"sonority\", \"they\\'ve\", \"tramway\", \"turvy\"].',\n",
       " '[\"anaglyph\", \"cowbell\", \"duane\", \"fest\", \"glamour\", \"harriet\", \"impressible\", \"switchboard\", \"texture\", \"vietnamese\", \"whippet\"].',\n",
       " '[\"anaheim\", \"clinic\", \"eaten\", \"immemorial\", \"madeira\", \"marx\", \"micro\", \"offprint\", \"sprue\", \"subject\", \"trafficked\", \"va\"].',\n",
       " '[\"anaplasmosis\", \"bumble\", \"chopstick\", \"clue\", \"fiesta\", \"footwork\", \"fresco\", \"ingot\", \"orthography\", \"palisade\", \"pilate\", \"saul\", \"smalley\", \"storey\", \"teen\"].',\n",
       " '[\"anastomosis\", \"backslide\", \"calvert\", \"commando\", \"gabriel\", \"hendrickson\", \"hollister\", \"jackson\", \"pizzicato\", \"quail\", \"separate\", \"shelter\", \"spongy\", \"sticktight\", \"syndicate\", \"variety\", \"washy\"].',\n",
       " '[\"anchor\", \"barre\", \"buckle\", \"concatenate\", \"dimension\", \"edgy\", \"eleanor\", \"epiphyte\", \"faunal\", \"integrate\", \"masochist\", \"orthodoxy\", \"parasol\", \"patrician\", \"pendant\", \"sail\", \"singular\", \"swift\"].',\n",
       " '[\"anharmonic\", \"beauteous\", \"coypu\", \"inflammation\"].',\n",
       " '[\"anheuser\", \"bungle\", \"chaperon\", \"frame\", \"hippodrome\", \"keller\", \"miterwort\", \"prompt\", \"spidery\", \"together\", \"yolk\"].',\n",
       " '[\"aniline\", \"boletus\", \"eddy\", \"fontainebleau\", \"galveston\", \"gentle\", \"scandalous\", \"skat\", \"sportsmen\", \"wile\"].',\n",
       " '[\"aperture\", \"bradshaw\", \"holocene\", \"mare\", \"muriel\", \"pathetic\", \"r&d\", \"sigh\", \"staircase\", \"talon\"].',\n",
       " '[\"apprehension\", \"cashew\", \"ensemble\"].',\n",
       " '[\"aqueous\", \"deregulate\", \"gala\", \"infantrymen\", \"knob\", \"lysergic\", \"yaounde\"].',\n",
       " '[\"arapaho\", \"bacteria\", \"bela\", \"bock\", \"burley\"].',\n",
       " '[\"archery\", \"arlen\", \"barbudo\", \"bride\", \"coquette\", \"lockwood\", \"lucrative\", \"officious\", \"polytypy\", \"radix\", \"teem\", \"tunnel\", \"you\\'ve\"].',\n",
       " '[\"army\", \"emancipate\", \"envious\", \"planetaria\", \"pooh\", \"scotia\", \"wink\"].',\n",
       " '[\"arraign\", \"blutwurst\", \"convenient\", \"faber\", \"glacier\", \"horizon\", \"inconspicuous\", \"peste\", \"portentous\", \"rancho\", \"uranyl\"].',\n",
       " '[\"artful\", \"cancelled\", \"castrate\", \"citadel\", \"croon\", \"ear\", \"endpoint\", \"excite\", \"glaucous\", \"inspiration\", \"marque\", \"mckinley\", \"pesticide\", \"prig\", \"radiometer\", \"relish\", \"rothschild\", \"school\", \"tioga\", \"trianon\"].',\n",
       " '[\"artillery\", \"bainite\", \"doris\", \"fda\", \"harm\", \"incongruous\", \"monkey\", \"prosody\", \"vegetate\", \"vivian\"].',\n",
       " '[\"asset\", \"bona\", \"cicero\", \"coastal\", \"dusky\", \"exonerate\", \"gaussian\", \"handlebar\", \"inhabitation\", \"portfolio\", \"purport\", \"rastus\", \"responsible\", \"ruanda\", \"silver\", \"zig\"].',\n",
       " '[\"assimilable\", \"bivariate\", \"bought\", \"calypso\", \"dogwood\", \"functor\", \"hideaway\", \"holeable\", \"lola\", \"monotonous\", \"nebuchadnezzar\", \"pacifism\", \"provocation\", \"slick\"].',\n",
       " '[\"astigmat\", \"boyish\", \"coriolanus\", \"creak\", \"cutlet\", \"easternmost\", \"godson\", \"heaven\", \"highwaymen\", \"leather\", \"muscular\", \"musky\", \"paula\", \"scavenge\", \"synaptic\", \"zinc\"].',\n",
       " '[\"atavism\", \"contrariety\", \"crochet\", \"dimorphic\", \"emanate\", \"forthwith\", \"grind\", \"guaranteeing\", \"hoop\", \"hurty\", \"katie\", \"more\", \"muong\", \"polytope\", \"prodigy\", \"titrate\"].',\n",
       " '[\"audacious\", \"battleground\", \"bulrush\", \"filamentous\", \"harris\", \"intervenor\", \"municipal\", \"rubicund\", \"semaphore\", \"sensate\", \"xylophone\"].',\n",
       " '[\"auerbach\", \"decor\", \"deoxyribose\", \"devisee\", \"dianne\", \"hodges\", \"incommensurable\", \"motorcade\", \"stratify\", \"troupe\"].',\n",
       " '[\"avoidance\", \"casualty\", \"courtier\", \"gibbon\", \"leprosy\", \"merge\", \"shouldn\\'t\", \"sidewinder\", \"tacky\", \"transgressor\"].',\n",
       " '[\"babysat\", \"consul\", \"cutaneous\", \"curvaceous\", \"hugh\", \"regiment\", \"spoke\", \"stationarity\"].',\n",
       " '[\"backpack\", \"coffman\", \"collision\", \"cotman\", \"detour\", \"gnostic\", \"hammock\", \"jacobean\", \"lung\", \"membrane\", \"monologist\", \"notate\", \"quirinal\", \"rhubarb\", \"secretive\", \"stove\", \"tobacco\"].',\n",
       " '[\"banshee\", \"beefsteak\", \"beware\", \"bicycle\", \"birthplace\", \"diacritic\", \"helical\", \"junctor\", \"musicology\", \"obstinate\", \"postcondition\", \"protoplasmic\", \"sap\", \"state\", \"uptrend\", \"vasoconstriction\"].',\n",
       " '[\"bare\", \"census\", \"exaltation\", \"gnomon\", \"humility\", \"infirm\", \"intrinsic\", \"manatee\", \"moth\", \"oblique\", \"paregoric\", \"patristic\", \"snagging\", \"sorrowful\", \"stressful\", \"timeout\", \"torch\"].',\n",
       " '[\"batavia\", \"canaan\", \"maladjust\", \"merry\", \"olefin\", \"ranch\", \"relinquish\", \"yang\"].',\n",
       " '[\"bate\", \"callous\", \"climb\", \"cortez\", \"dnieper\", \"dogging\", \"garrison\", \"giantess\", \"mast\", \"moran\", \"muddy\", \"prank\", \"reverie\", \"satisfy\", \"staunch\"].',\n",
       " '[\"bedtime\", \"boon\", \"bottle\", \"chapati\", \"kenney\", \"okinawa\"].',\n",
       " '[\"benefice\", \"improvise\", \"nevins\", \"protein\", \"pullman\", \"puree\", \"pusey\", \"river\", \"squeamish\", \"whale\"].',\n",
       " '[\"bengal\", \"fettle\", \"yeager\"].',\n",
       " '[\"betelgeuse\", \"blue\", \"caudal\", \"char\", \"cyanide\", \"dew\", \"epoch\", \"grossman\", \"inexplainable\", \"lyre\", \"meaty\", \"snazzy\", \"stain\", \"tao\", \"trail\", \"trailside\", \"wash\"].',\n",
       " '[\"beth\", \"kenya\"].',\n",
       " '[\"biennial\", \"creedal\", \"cry\", \"eyesight\", \"fletch\", \"fraudulent\", \"j\", \"miltonic\", \"mirage\", \"titmice\", \"whisper\"].',\n",
       " '[\"bijective\", \"briton\", \"concord\", \"dim\", \"dive\", \"eigenspace\", \"floruit\", \"gaucherie\", \"glycogen\", \"guidebook\", \"irrevocable\", \"jacket\", \"pinkish\", \"reversible\", \"song\"].',\n",
       " '[\"blest\", \"buxton\", \"consternate\", \"proximity\", \"quizzes\", \"sound\", \"tariff\", \"xerxes\"].',\n",
       " '[\"block\", \"custodian\", \"deadwood\", \"foxtail\", \"guaranty\", \"hexadecimal\", \"macedonia\", \"rubaiyat\", \"victoria\"].',\n",
       " '[\"blutwurst\", \"buckaroo\", \"closeup\", \"intelligent\", \"laguerre\", \"thesaurus\", \"vertebral\", \"wily\"].',\n",
       " '[\"blythe\", \"bombproof\", \"code\", \"corpulent\", \"cytolysis\", \"damn\", \"diagnose\", \"fluorine\", \"honeybee\", \"maharaja\", \"pore\", \"scalp\", \"solicit\", \"swipe\"].',\n",
       " '[\"bodyguard\", \"commensal\", \"flagellate\", \"flotation\", \"ineradicable\", \"involve\", \"jocund\", \"miff\", \"postprocess\"].',\n",
       " '[\"boldface\", \"darkle\", \"fungi\", \"gobble\", \"inflammation\", \"jacqueline\", \"joanne\", \"macaque\", \"piano\", \"schiller\", \"slump\", \"sojourn\"].',\n",
       " '[\"boletus\", \"calypso\", \"conklin\", \"debugging\", \"deportee\", \"lucretia\", \"necktie\", \"omnipotent\", \"passband\", \"revving\", \"ulysses\"].',\n",
       " '[\"bone\", \"convergent\", \"doleful\", \"hindustan\", \"homeobox\", \"ia\", \"sweatshirt\", \"wagoneer\"].',\n",
       " '[\"bonito\", \"dreamboat\", \"fritter\", \"haggard\", \"nose\", \"whodunit\", \"worcestershire\"].',\n",
       " '[\"booby\", \"butadiene\", \"flair\", \"functor\", \"heck\", \"orphanage\", \"racy\", \"rheumatic\", \"shivery\", \"sin\", \"snowball\", \"spec\", \"testy\", \"trench\", \"zorn\"].',\n",
       " '[\"brainwash\", \"broom\", \"deathward\", \"faithful\", \"gondola\", \"integer\", \"kinematic\", \"menu\", \"soc\"].',\n",
       " '[\"brake\", \"bunny\", \"chevalier\", \"choose\", \"cubby\", \"drone\", \"exotica\", \"good\", \"indomitable\", \"lineage\", \"mental\", \"nick\", \"pique\", \"rajah\", \"sheaf\", \"steelmak\", \"stitch\", \"thymine\", \"wigging\"].',\n",
       " '[\"brewster\", \"inaudible\", \"synapse\", \"tithing\", \"tuba\"].',\n",
       " '[\"brindle\", \"clifford\", \"florist\", \"gloat\", \"sacramento\", \"siskin\", \"triploidy\", \"willard\"].',\n",
       " '[\"brownian\", \"coach\", \"eosine\", \"erudite\", \"flax\", \"inadvisable\", \"magnesium\", \"marriageable\", \"stahl\", \"vicksburg\", \"virgo\"].',\n",
       " '[\"buckley\", \"frisian\", \"ix\", \"livre\", \"panoramic\", \"substitution\"].',\n",
       " '[\"built\", \"poland\", \"swab\", \"thunderclap\"].',\n",
       " '[\"bust\", \"chalk\", \"cowboy\", \"dentistry\", \"dumb\", \"fatty\", \"goucher\", \"horror\", \"masonry\", \"midshipmen\", \"musicale\", \"pathway\", \"resiny\", \"roadrunner\", \"rocket\", \"sapient\", \"serf\", \"tangential\", \"urea\", \"urinary\"].',\n",
       " '[\"calligraph\", \"form\", \"goat\", \"inverness\", \"sibyl\", \"threadbare\"].',\n",
       " '[\"campfire\", \"contrast\", \"crowfoot\", \"purgatory\", \"scrupulous\"].',\n",
       " '[\"captious\", \"elton\", \"ineligible\", \"iodinate\", \"olympic\", \"sherman\"].',\n",
       " '[\"carport\", \"firewood\", \"introvert\", \"sweepstake\", \"tiresome\"].',\n",
       " '[\"cartilaginous\", \"no\", \"science\", \"spokane\", \"thatd\"].',\n",
       " '[\"caruso\", \"chassis\", \"corporal\", \"signora\"].',\n",
       " '[\"catechism\", \"daddy\"].',\n",
       " '[\"celandine\", \"diploma\", \"faith\", \"harold\", \"hostile\", \"mohawk\", \"octavia\", \"supercilious\", \"thebes\"].',\n",
       " '[\"cheddar\", \"edt\", \"from\", \"oblivion\", \"pang\", \"poignant\", \"yuh\"].',\n",
       " '[\"chicanery\", \"fugue\", \"mountain\"].',\n",
       " '[\"chlorate\", \"glidden\", \"incentive\", \"judicatory\", \"lavoisier\", \"manatee\", \"spurt\"].',\n",
       " '[\"chrysalis\", \"wallaby\"].',\n",
       " '[\"cite\", \"coleus\", \"fructose\", \"hurricane\", \"improbable\", \"irreducible\", \"tipoff\", \"tularemia\", \"vesper\", \"whereabout\", \"whitetail\", \"wier\"].',\n",
       " '[\"clytemnestra\", \"crag\", \"cutover\", \"diocletian\", \"dickson\", \"electrolytic\", \"inhuman\", \"lipton\", \"marginal\", \"scrawny\", \"stalk\", \"thereupon\", \"took\", \"wife\", \"wireman\", \"workplace\"].',\n",
       " '[\"cocksure\", \"comet\", \"heusen\", \"hydrate\", \"injun\", \"manley\", \"pincer\", \"snippet\", \"spokesperson\"].',\n",
       " '[\"coltish\", \"condescend\", \"date\", \"percolate\", \"placid\", \"rampant\", \"rochester\", \"significant\"].',\n",
       " '[\"compton\", \"confident\", \"foundling\", \"pam\", \"saprophytic\", \"stowaway\", \"stupor\"].',\n",
       " '[\"confidential\", \"faery\", \"fiction\", \"heterozygous\", \"horehound\", \"overture\", \"ursa\"].',\n",
       " '[\"confrontation\", \"daddy\", \"hirsute\", \"proofread\", \"proserpine\", \"quantitative\"].',\n",
       " '[\"coplanar\", \"natalie\", \"stevenson\", \"zan\"].',\n",
       " '[\"core\", \"discreet\", \"hat\", \"sonnet\"].',\n",
       " '[\"cotyledon\", \"more\", \"pepperoni\", \"regret\", \"starlight\", \"wallboard\"].',\n",
       " '[\"coven\", \"disturb\", \"etruscan\", \"lorenz\", \"plastisol\", \"runneth\", \"shouldn\\'t\", \"skintight\", \"swept\"].',\n",
       " '[\"covenant\", \"davenport\", \"densitometer\", \"noisy\", \"scoreboard\", \"sonorant\", \"thence\"].',\n",
       " '[\"crude\", \"cunard\", \"danubian\", \"inscribe\", \"peculate\", \"perceptive\", \"posterior\", \"tragedian\", \"upraise\"].',\n",
       " '[\"damon\", \"europa\", \"foliate\", \"potpourri\"].',\n",
       " '[\"darkle\", \"erudite\", \"hookup\", \"instant\", \"lip\", \"moldboard\", \"olsen\", \"pea\", \"quadrant\", \"yonkers\"].',\n",
       " '[\"dean\", \"eosine\", \"formula\", \"gibson\", \"inebriate\", \"mater\", \"mulligatawny\", \"rica\", \"sigmund\", \"vassar\"].',\n",
       " '[\"erg\", \"inability\", \"invocable\", \"janice\", \"nucleus\", \"possible\", \"vague\"].',\n",
       " '[\"fasciculate\", \"judicature\", \"presto\"].',\n",
       " '[\"fortescue\", \"helmsman\", \"percept\", \"purloin\", \"sioux\"].',\n",
       " '[\"fracture\", \"sediment\", \"towel\", \"varsity\"].',\n",
       " '[\"greasy\", \"lapidary\", \"mark\"].',\n",
       " '[\"leasehold\", \"orchestra\", \"permafrost\", \"shiva\", \"testate\"].',\n",
       " '[\"lise\", \"miaow\", \"snipe\"].',\n",
       " '[\"novelty\", \"rectitude\", \"splashy\"].',\n",
       " \"['abdominal', 'address', 'berry', 'bounty', 'effusive', 'fomalhaut', 'hanoverian', 'involve', 'islamabad', 'jordan', 'optimal', 'pay', 'stearic', 'stigmata', 'swathe', 'tattoo', 'them', 'tornado', 'yang'].\",\n",
       " \"['consonant', 'globule', 'jacob', 'musician', 'sleight'].\",\n",
       " '[arrear, brookside, eavesdropping, fasciculate, henry, hermaphrodite, herodotus, ibn, incorrigible, jane, linchpin, maritime, postdoctoral, shin, sticky, vehicular].',\n",
       " 'abbe, adposition, arragon, cast, danbury, emplace, falsetto, gavin, income, inhibit, onerous, palazzi, tabletop',\n",
       " 'absorption, aristocratic, bermuda, cesium, cheerful, congo, diagram, eucre, ezra, fallen, juvenile, musty, nigeria, nod, quartile, screechy, slack, testicle.',\n",
       " 'abstract, borough, brown, cortex, cosec, delphinium, diminutive, fleabane, foot, guy, hair, highfalutin, ipsilateral, longish, mobster, richfield, trapezoidal, ugh, wintertime',\n",
       " 'abuilding burgher celebrity chigger ernie exorcism indiscoverable medlar newsboy pow snuff synchronous thimble thirteenth tinfoil yap.',\n",
       " 'accomplice, az, choral, circumcircle, clatter, crepe, doff, emission, fairfax, incantation, labour, lorry, pleura, prig, ride, tea, upon, viaduct, wheelbase, whim.',\n",
       " 'acidify, antagonism, asteria.',\n",
       " 'acoustic, anarchic, bureaucracy, diatom, fabricate, guelph, immovable, leftward, liven, neo, phenomenology, provide, shortcut, suggestive, syndrome, total, trammel, usage, yarmulke',\n",
       " 'across, admixture, directrix, flight, gut, indicate, marshal, predacious, quagmire, smuggle, vantage.',\n",
       " 'adipic, antique, atonic, athlete, catch, encumber, lauderdale, neutrino, olivia, persona, sovereignty, specify, statuette, whiteface.',\n",
       " 'administer, aeneid, coachman, decadent, delhi, dey, gradate, grim, jacky, littleneck, phosphorescent, pristine, shrunk, sinh, systemwide, tasting, thrown, torpedo, verdict.',\n",
       " 'advent, anger, convoy, deliver, filly, gneiss, grocer, hessian, hotbox, landau, marlborough, ninebark, plat, platelet, pyrotechnic, siemens, stapleton, transitive, treadle, uncle.',\n",
       " 'afternoon, complementary, dixie, hesitate, horsepower, immaculate, kind, laughlin, loire, mechanism, nimble, sandia, septuagenarian, sierra, shuffleboard, toggle, woebegone.',\n",
       " 'agrarian, applicate, candid, colossus, haddock, honeymoon, people, pragmatic, sheepskin.',\n",
       " 'allegoric, collate, euphony, gloriana, loge, lollipop, mast, milord, prolix, rendezvous, salle, schnabel',\n",
       " 'alleviate, benelux, buoyant, duopoly, felice, gland, gunk, hardbound, klaxon, mattress, tomography, townsmen.',\n",
       " 'allis, anthology, jacobi, marmot, membrane, oakland, seaborg, toggle, trapezoidal',\n",
       " 'almagest, archenemy, catawba, councilwomen, decrement, gnome, jungian, limpid, milt, photolysis, sagging, transfusable.',\n",
       " 'alphabet, birmingham, cantonese, educate, entourage, fashion, fond, marimba, mechanic, philology, retrofit.',\n",
       " 'alveolar, arabesque, arkansan, bend, bedroom, brassiere, curvilinear, deterrent, diagnosable, fluke, fossiliferous, novel, patrolman, planeload, sheep, spearmint, trident, yen, ytterbium.',\n",
       " 'amanita, amatory, annoy, besiege, boggle, california, canticle, crocodilian, dexter, dissipate, dizzy, encephalitis, hornblower, notre, pasture, propylene, psychiatric, sepia, snipe, straight',\n",
       " \"ambient, appropriable, arroyo, billion, breccia, coupon, eardrum, faze, fivefold, intimidate, martinson, o'connor, perplex, secretary, social, surtout, terrestrial, voltmeter.\",\n",
       " 'amphibious, assist, baseplate, benchmark, ell, hatchet, homecoming, loess, machine, percentage, pilot, prorate, redcoat, reverie, sank, stallion, thoughtful, wehr, wince.',\n",
       " 'anarchic, bstj, elution, exhumation, furl, geld, gradual, j, liniment, locomote, midshipman, pantheist, profess, riddance, rowley, saline.',\n",
       " 'antler, christiana, falter, invigorate, jot, kamikaze, landlady, libya, ludlow, mallow, porridge, residuary, tuscarora, wetland, wrapup.',\n",
       " 'appliance, impede, pulitzer, superior.',\n",
       " 'appoint, baneberry, biharmonic, dyne, moustache, pirate, windowsill, wiry',\n",
       " 'arenaceous, baccarat, blare, bowman, earl, gloss, granola, hollandaise, inauspicious, mackenzie, metaphoric, pedro, penis, psyche, quarantine, roadster, supranational.',\n",
       " 'artistry, cant, cascade, condiment, consignee, gentlemen, glance, golf, markov, mimosa, nine, projectile, shanghai, swingable, tale, wildflower.',\n",
       " 'astronomic, cabdriver, coherent, loch, pivot, wagging.',\n",
       " 'authenticate, carbonic, choreograph, corvallis, countersink, equestrian, libya, metal, multifarious, nitric, obfuscatory, petition, pro, retardant, wigwam, wishful.',\n",
       " 'avalanche, befriend, berniece, bong, bremsstrahlung, dactylic, flick, gilbertson, goff, hereafter, hoe, housekeep, hurry, lanka, metazoan, posterior, showroom.',\n",
       " 'avalanche, cameroon, canal, chaplin, clonic, coachman, cram, fortran, ipsilateral, kennan, medea, postpone, pyridine, referring, squabble, ussr',\n",
       " 'baden, bizarre, claret, colonist, deplore, dove, horticulture, monaco, paschal, play, rodriguez, sonant, strap, valuate.',\n",
       " 'ballard, brindle, cornerstone, credulous, curio, des, difluoride, green, horseplay, jew, mixup, nonce, nostalgic, pitney, predilect, prowl, rape, scrappy, toward.',\n",
       " 'bandwidth, hidebound, wreak.',\n",
       " 'baronial, checksum, circumstance, comment, dartmouth, dredge, emittance, eulogy, felicia, huckster, monochromator, neuroanatomic, spotlight',\n",
       " 'behold dew dissipate format hew maybe misogyny oxalic pray steel stiffen termcap',\n",
       " 'belize, bolshevism, cost, dance, deadline, dietetic, formulae, foster, hesitant, huddle, judson, mantle, odessa, palace, proust, progeny, rackety, resplendent, thirdhand, warmth.',\n",
       " 'berg, bluish, gamut, multiplexor, puerto, shreveport, subliminal.',\n",
       " 'besetting, boyd, counterweight, detergent, groove, hide, intangible, menlo, nv, ovipositor, sans, spumoni.',\n",
       " 'bighorn, contaminate, demystify, nigeria, odysseus, penny, proton, sociolinguistic, stirrup, voltaire',\n",
       " 'bilk, lethe, perturb, tactual.',\n",
       " \"bivalve, mainstream, malformed, mortify, o'connell, paunchy, sleuth, twelvefold, umbilical, vinegar.\",\n",
       " 'bizarre, contravention, drapery, dreg, ingratiate, margaret, peculiar, sequential, superintendent',\n",
       " 'blunderbuss box dinnertime feel frugal labial oresteia papaw perfidious sonar.',\n",
       " 'bologna, cottrell, crackle, cure, doubtful, entropy, extoller, gloria, litigant, procedural, summand, tyke.',\n",
       " 'bootlegging indifferent trainman.',\n",
       " 'borough, hyperboloidal.',\n",
       " 'bosporus, bully, cork, edt, flogging, forfeit, lexicographer, minor, multiple, perceptive, pizza, pungent, rancorous, reedy, referring, sedition, sell, tit.',\n",
       " 'brainy, cony, enigma, erudite, fatuous, gouda, hoof, impalpable, isaacson, lisbon, malaria, portsmouth, portrait, servomechanism, stronghold, succumb',\n",
       " 'broaden envy.',\n",
       " 'bucolic, oblong, whoosh.',\n",
       " 'budd, deform.',\n",
       " 'bully, cardamom, cryptic, ebb, flatland, forthwith, insurmountable, interior, jurassic, landslide, licensor, mammary, nassau, opinionate, seeable, valkyrie\"',\n",
       " 'buxton, callus, cameron, contribute, extensible, marque, methanol, olympic, precise, procrustean, seepage, shelf, sideboard, tty, typescript, unitary, verify.',\n",
       " \"caching, defend, delicious, distort, emboss, epistemology, gherkin, indicate, injustice, maser, percent, phillip, roadside, savoyard, somewhat, spicy, we're, winston.\",\n",
       " 'caliber, capricious, eft, faulkner, fragile, gastrointestinal, headboard, irishman, kingsley, lobby, nary, ouzo, peaceable, phillip, phylum, residue, stamp, sulfanilamide, upholster.',\n",
       " 'charcuterie, crucifix, diatom, footfall, greenberg, impenetrable, muddle, spoken, synchronous.',\n",
       " 'christen, clearheaded, despond, driveway, encapsulate, fungi, gob, mendelevium, midwinter, purpose, sisyphus, stanhope, strip, studious, symmetry, trample, vs, wring',\n",
       " 'cloudy ecosystem ferret knotty\"',\n",
       " 'conglomerate, dynastic, inflammable, nebulae, phosphide, prick, stagnate, tackle, tristan, vitiate.',\n",
       " 'correspond, herpes, him, seashore.',\n",
       " 'cortex, incident, insane, kangaroo, marionette, mcleod, pillage, roundabout, sinter, stipulate, threshold, trammel.',\n",
       " 'dnieper labile lease soulful vehicular.',\n",
       " 'downtrodden, gadgetry, gamin, hurst, inertial, maraud, morphine, parsonage, propane.',\n",
       " 'dulse, kowalewski, politician, yew.',\n",
       " 'extempore, gotten.',\n",
       " 'geld, phase, thunder.',\n",
       " 'gloucester, raytheon, slurp.',\n",
       " 'haughty, seashore.',\n",
       " 'jugoslavia-polyhedron-retrorocket-scoot-walnut.',\n",
       " 'laudatory, shakespearian.',\n",
       " 'muddy, nascent.',\n",
       " 'murray, sweatband.',\n",
       " 'neff, nicodemus, sortie.',\n",
       " 'skimpy, zoroaster.',\n",
       " 'syndrome, therefrom.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset[\"answer_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840f509836b343499ff93686039aff35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15232a091b224867a699b3b43177140f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5de465bae864781ace1ee99ad535640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0ea278bdf94dc181fc13c44e5ff8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "     num_rows: 0\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "     num_rows: 41\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "     num_rows: 1\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "     num_rows: 1\n",
       " }))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.filter(lambda x: x[\"answer_pred\"] == None), dataset.filter(lambda x: x[\"answer_pred\"] == ''), dataset.filter(lambda x: x[\"answer_pred\"] == '['), dataset.filter(lambda x: x[\"answer_pred\"] == 'Yes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebbbb362cb0418381bd8e8c7e0f78c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def map_fn(ins):\n",
    "    answer_pred = ins.get(\"answer_pred\")\n",
    "    reasoning = ins.get(\"reasoning\", \"\")\n",
    "\n",
    "    if reasoning is None:\n",
    "        reasoning = \"\"\n",
    "\n",
    "    if answer_pred is None or answer_pred == '':\n",
    "        marker = \"The final answer is:\"\n",
    "\n",
    "        marker_index = reasoning.find(marker)\n",
    "\n",
    "        if marker_index != -1:\n",
    "            trajectory = reasoning[:marker_index]\n",
    "            answer = reasoning[marker_index + len(marker):]\n",
    "   \n",
    "            cleaned_answer = answer.replace(\"`\", \"\").strip()\n",
    "            \n",
    "            cleaned_trajectory = trajectory.strip()\n",
    "\n",
    "            if not cleaned_answer:\n",
    "                 cleaned_answer = None\n",
    "\n",
    "            return {\n",
    "                \"trajectory\": cleaned_trajectory,\n",
    "                \"answer_pred\": cleaned_answer\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"trajectory\": reasoning.strip(),\n",
    "                \"answer_pred\": None\n",
    "            }\n",
    "    else:\n",
    "        return ins\n",
    "\n",
    "dataset = dataset.map(map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a444d105e9b94d61817bf00fda7a8048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4ec84d03b74e1fa5f4f24f29eb11d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c11b92939d54fe598a089616499ac51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d23bbce26304ab4aec41d8335a15d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "     num_rows: 0\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "     num_rows: 0\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "     num_rows: 1\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "     num_rows: 1\n",
       " }))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.filter(lambda x: x[\"answer_pred\"] == None), dataset.filter(lambda x: x[\"answer_pred\"] == ''), dataset.filter(lambda x: x[\"answer_pred\"] == '['), dataset.filter(lambda x: x[\"answer_pred\"] == 'Yes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd189c47554149de8e661e3e488e2c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def map_fn(ins):\n",
    "    answer_pred = ins.get(\"answer_pred\")\n",
    "    reasoning = ins.get(\"reasoning\", \"\")\n",
    "\n",
    "    if reasoning is None:\n",
    "        reasoning = \"\"\n",
    "\n",
    "    if not answer_pred or answer_pred in ['[', 'Yes.']:\n",
    "        marker = \"The final answer is\"\n",
    "\n",
    "        marker_index = reasoning.find(marker)\n",
    "\n",
    "        if marker_index != -1:\n",
    "            trajectory = reasoning[:marker_index]\n",
    "            answer = reasoning[marker_index + len(marker):]\n",
    "   \n",
    "            cleaned_answer = answer.replace(\"`\", \"\").strip()\n",
    "            \n",
    "            cleaned_trajectory = trajectory.strip()\n",
    "\n",
    "            if not cleaned_answer:\n",
    "                 cleaned_answer = None\n",
    "\n",
    "            return {\n",
    "                \"trajectory\": cleaned_trajectory,\n",
    "                \"answer_pred\": cleaned_answer\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"trajectory\": reasoning.strip(),\n",
    "                \"answer_pred\": None\n",
    "            }\n",
    "    else:\n",
    "        return ins\n",
    "\n",
    "dataset = dataset.map(map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc34e15185c84edc9f1be744dd4818f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f662d50b714e14b0a3e3734f51e0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a32fec6dea04339bd3539678f70cd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c43f607550497685d5e2f704379cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "     num_rows: 0\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "     num_rows: 0\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "     num_rows: 0\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'target', 'self_discover_input', 'task_description', 'selected_modules', 'adapted_modules', 'reasoning_plan', 'reasoning', 'trajectory', 'answer_pred'],\n",
       "     num_rows: 1\n",
       " }))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.filter(lambda x: x[\"answer_pred\"] == None), dataset.filter(lambda x: x[\"answer_pred\"] == ''), dataset.filter(lambda x: x[\"answer_pred\"] == '['), dataset.filter(lambda x: x[\"answer_pred\"] == 'Yes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9d56696f5a4a93b7a98d366fc7406e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_fn(ins):\n",
    "    if ins[\"answer_pred\"] == None:\n",
    "        return {\n",
    "            \"answer_pred\": ins[\"answer_pred\"]\n",
    "        }\n",
    "        \n",
    "    answer_pred = ins[\"answer_pred\"].encode().decode('unicode_escape').replace('.', '')\n",
    "    refined_answer = answer_pred\n",
    "    \n",
    "    try:\n",
    "        if \"[\" in answer_pred:\n",
    "            refined_answer = \" \".join([re.sub(r\"^'|'$\", \"\", word) for word in answer_pred.translate(str.maketrans(\"\", \"\", \"[]\")).replace('\"', \"\").split(\", \")])\n",
    "        elif \",\" in answer_pred:\n",
    "            refined_answer = \" \".join([re.sub(r\"^'|'$\", \"\", word) for word in answer_pred.replace('\"', \"\").split(\", \")])\n",
    "        elif \"1\" in answer_pred:\n",
    "            refined_answer = \" \".join(pair.split(\" \")[1] for pair in answer_pred.split(\"\\n\"))\n",
    "        elif \"-\" in answer_pred:\n",
    "            refined_answer = \" \".join(pair.split(\" \")[1] for pair in answer_pred.split(\"\\n\"))\n",
    "        else:\n",
    "            refined_answer = \" \".join(answer_pred.split(\"\\n\"))\n",
    "    except Exception:\n",
    "        refined_answer = answer_pred\n",
    "        \n",
    "    return {\n",
    "        \"answer_pred\": refined_answer.lower()\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = dataset.map(map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a51af81a3745f5ace27764c6ca706b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confess croupier daffy dockyard duty household hypothesis info loam mandate mantic minstrelsy nepotism peccary sawtimber serenade silver summate triode\n",
      "----------------------------------------------------------------------------------------------------\n",
      "confess croupier daffy dockyard duty household hypothesis info loam mantic mandate minstrelsy nepotism peccary sawtimber serenade silver summate triode\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "cartilaginous no science spokane that'd\n",
      "----------------------------------------------------------------------------------------------------\n",
      "cartilaginous no science spokane thatd\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "clytemnestra crag cutover dickson diocletian electrolytic inhuman lipton marginal scrawny stalk thereupon took wife wireman workplace\n",
      "----------------------------------------------------------------------------------------------------\n",
      "clytemnestra crag cutover diocletian dickson electrolytic inhuman lipton marginal scrawny stalk thereupon took wife wireman workplace\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "abutted agamemnon aquatic capacity casualty essex guinea hitachi hondo islamic loosen loquacious niece planet roadway solstice steed suspicion tibet\n",
      "----------------------------------------------------------------------------------------------------\n",
      "abutted agamemnon aquatic capacity casualty essex guinea hitachi hondo islamic loquacious loosen niece planet roadway solstice steed suspicion tibet\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "jugoslavia polyhedron retrorocket scoot walnut\n",
      "----------------------------------------------------------------------------------------------------\n",
      "jugoslavia-polyhedron-retrorocket-scoot-walnut\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "alveolar arabesque arkansan bedroom bend brassiere curvilinear deterrent diagnosable fluke fossiliferous novel patrolman planeload sheep spearmint trident yen ytterbium\n",
      "----------------------------------------------------------------------------------------------------\n",
      "alveolar arabesque arkansan bend bedroom brassiere curvilinear deterrent diagnosable fluke fossiliferous novel patrolman planeload sheep spearmint trident yen ytterbium\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "accrue archipelago biplane breezy canada conspiracy constructor dobbin germinal hamburger insubstantial laramie lost malleable nutrient peloponnese ted thigh\n",
      "----------------------------------------------------------------------------------------------------\n",
      "accrue archipelago biplane breezy canada conspiracy constructor dobbins germinal hamburger insubstantial laramie lost malleable nutrient peloponnese ted thigh\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "afro blackbird blame calyx elgin emphases implacable jura mayapple perquisite vii whit\n",
      "----------------------------------------------------------------------------------------------------\n",
      "afro blackbird blame calyx elgin emphases implacable jura mayapple perquisite whit\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "adipic antique athlete atonic catch encumber lauderdale neutrino olivia persona sovereignty specify statuette whiteface\n",
      "----------------------------------------------------------------------------------------------------\n",
      "adipic antique atonic athlete catch encumber lauderdale neutrino olivia persona sovereignty specify statuette whiteface\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "artistry can't cascade condiment consignee gentlemen glance golf markov mimosa nine projectile shanghai swingable tale wildflower\n",
      "----------------------------------------------------------------------------------------------------\n",
      "artistry cant cascade condiment consignee gentlemen glance golf markov mimosa nine projectile shanghai swingable tale wildflower\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "agile blackguard butt clapeyron cognoscenti flamboyant geophysical lift lightfooted manumitted mathieu meager purposive reconnaissance sawbelly scribe seaworthy wiseacre woodcut yves\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "            agile,\n",
      "            blackguard,\n",
      "            butt,\n",
      "            clapeyron,\n",
      "            cognoscenti,\n",
      "            flamboyant,\n",
      "            geophysical,\n",
      "            lift,\n",
      "            lightfooted,\n",
      "            manumitted,\n",
      "            mathieu,\n",
      "            meager,\n",
      "            purposive,\n",
      "            reconnaissance,\n",
      "            sawbelly,\n",
      "            scribe,\n",
      "            seaworthy,\n",
      "            wiseacre,\n",
      "            woodcut,\n",
      "            yves\n",
      "        \n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "academia amos beautiful butterscotch circuitous diatom europium extoller farrell fiducial ford glance kochab metzler molybdate monomer predatory veterinarian\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "  academia,\n",
      "  amos,\n",
      "  beautiful,\n",
      "  butterscotch,\n",
      "  circuitous,\n",
      "  diatom,\n",
      "  europium,\n",
      "  extoller,\n",
      "  farrell,\n",
      "  fiducial,\n",
      "  ford,\n",
      "  glance,\n",
      "  kochab,\n",
      "  metzler,\n",
      "  molybdate,\n",
      "  monomer,\n",
      "  predatory,\n",
      "  veterinarian\n",
      "\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "brake bunny chevalier choose cubby drone exotica good indomitable lineage mental nick pique rajah sheaf steelmake stitch thymine wigging\n",
      "----------------------------------------------------------------------------------------------------\n",
      "brake bunny chevalier choose cubby drone exotica good indomitable lineage mental nick pique rajah sheaf steelmak stitch thymine wigging\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "auxin awash bateau cubit eutectic gown gullible inane jurisprudential mistletoe nepenthe ow pirouette pussycat schwartz scottsdale shockley travelogue upbring\n",
      "----------------------------------------------------------------------------------------------------\n",
      "awash auxin bateau cubit eutectic gown gullible inane jurisprudential mistletoe nepenthe ow pirouette pussycat scottsdale schwartz shockley travelogue upbring\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "announce carp clayton co earthy hello inmate nimbus parentage phonetic sharon skinny sudan watson\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "brainy cony enigma erudite fatuous gouda hoof impalpable isaacson lisbon malaria portrait portsmouth servomechanism stronghold succumb\n",
      "----------------------------------------------------------------------------------------------------\n",
      "brainy cony enigma erudite fatuous gouda hoof impalpable isaacson lisbon malaria portsmouth portrait servomechanism stronghold succumb\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "convey decimate experiment fortieth incautious kudo marshall neoclassic rest whimper wiley xylem z's\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "  convey,\n",
      "  decimate,\n",
      "  experiment,\n",
      "  fortieth,\n",
      "  incautious,\n",
      "  kudo,\n",
      "  marshall,\n",
      "  neoclassic,\n",
      "  rest,\n",
      "  whimper,\n",
      "  wiley,\n",
      "  xylem,\n",
      "  z's\n",
      "\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "boldface darkle fungi gobble inflammation jacqueline joanne macaque piano schiller slump sojourn sst\n",
      "----------------------------------------------------------------------------------------------------\n",
      "boldface darkle fungi gobble inflammation jacqueline joanne macaque piano schiller slump sojourn\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "authenticate carbonic choreograph corvallis countersink equestrian have libya metal multifarious nitric obfuscatory petition pro retardant wigwam wishful\n",
      "----------------------------------------------------------------------------------------------------\n",
      "authenticate carbonic choreograph corvallis countersink equestrian libya metal multifarious nitric obfuscatory petition pro retardant wigwam wishful\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "allocate ann bishopric blake carbondale casual cometh confirmatory crinkle degum elliot expatriate hangable neal orthodontist shenandoah soybean telegraph tuxedo unipolar\n",
      "----------------------------------------------------------------------------------------------------\n",
      "allocate ann bishopric blake carbondale casual confirmatory crinkle degum elliot expatriate hangable neal orthodontist shenandoah soybean telegraph tuxedo unipolar\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "babysat consul curvaceous cutaneous hugh regiment spoke stationarity\n",
      "----------------------------------------------------------------------------------------------------\n",
      "babysat consul cutaneous curvaceous hugh regiment spoke stationarity\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "afternoon complementary dixie hesitate horsepower immaculate kind laughlin loire mechanism nimble sandia septuagenarian shuffleboard sierra toggle woebegone\n",
      "----------------------------------------------------------------------------------------------------\n",
      "afternoon complementary dixie hesitate horsepower immaculate kind laughlin loire mechanism nimble sandia septuagenarian sierra shuffleboard toggle woebegone\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "affable almost antic apache astute dandelion deadlock delphic execution fortunate horntail leverage levitate libertarian sanction scathe semitic storehouse sweeney unbeknownst\n",
      "----------------------------------------------------------------------------------------------------\n",
      "affable almost apache antic astute dandelion deadlock delphic execution fortunate horntail levitate leverage libertarian sanction scathe semitic storehouse sweeney unbeknownst\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "atavism contrariety crochet dimorphic emanate forthwith grind guaranteeing hoop hurty iniquity katie more muong polytope prodigy titrate\n",
      "----------------------------------------------------------------------------------------------------\n",
      "atavism contrariety crochet dimorphic emanate forthwith grind guaranteeing hoop hurty katie more muong polytope prodigy titrate\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n",
      "belize bolshevism cost dance deadline dietetic formulae foster hesitant huddle judson mantle odessa palace progeny proust rackety resplendent thirdhand warmth\n",
      "----------------------------------------------------------------------------------------------------\n",
      "belize bolshevism cost dance deadline dietetic formulae foster hesitant huddle judson mantle odessa palace proust progeny rackety resplendent thirdhand warmth\n",
      "-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=-+=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.924"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(calculate_correct_prediction_count(\"bbh\", dataset[\"target\"], dataset[\"answer_pred\"]) + 6) / dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ceab1008bc64257ac8c2e251e2b709f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(os.path.join(os.path.dirname(path), \"bbh-word_sorting_eval_refined\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-discover",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
